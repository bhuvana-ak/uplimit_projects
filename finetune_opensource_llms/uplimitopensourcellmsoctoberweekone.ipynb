{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring LLM Dataset Creation and Evaluation\n",
        "\n",
        "## Install the required libraries"
      ],
      "metadata": {
        "id": "cNxVuXtVuc9c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqxvscIaXsX8"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "pip install -qqq huggingface-hub argilla \"distilabel[huggingface]\" accelerate openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import argilla as rg\n",
        "from google.colab import userdata\n",
        "\n",
        "# Authenticate with Argilla\n",
        "dataset = load_dataset(\"DIBT/10k_prompts_ranked\")\n",
        "column_names = dataset[\"train\"].column_names\n",
        "print(column_names)"
      ],
      "metadata": {
        "id": "ih2qY7rEampt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][0]"
      ],
      "metadata": {
        "id": "1-1FVMiPdZrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argilla as rg\n",
        "\n",
        "# Initialize Argilla client\n",
        "api_url = userdata.get('ARGILLA_API_URL')  # or os.environ if you are not using Google Colab\n",
        "api_key = userdata.get(\"ARGILLA_API_KEY\")  # or os.environ if you are not using Google Colab\n",
        "workspace = \"admin\"\n",
        "dataset_name = \"DIBT_10k_prompts\"\n",
        "\n",
        "# Initialize the Argilla client\n",
        "client = rg.Argilla(api_url=api_url, api_key=api_key)\n",
        "\n",
        "# Create a new Dataset\n",
        "dataset = rg.Dataset(\n",
        "    name=dataset_name,\n",
        "    workspace=workspace,\n",
        "    client=client\n",
        ")\n",
        "\n",
        "# Configure the dataset settings\n",
        "dataset.settings.fields = [\n",
        "    rg.TextField(name=\"id\"),\n",
        "    rg.TextField(name=\"instruction\"),\n",
        "    rg.TextField(name=\"generation\"),\n",
        "]\n",
        "\n",
        "dataset.settings.questions = [\n",
        "    rg.LabelQuestion(\n",
        "        name=\"quality\",\n",
        "        labels=[\"👎\", \"👍\"],\n",
        "        title=\"Quality of the generated text\",\n",
        "    )\n",
        "]\n",
        "\n",
        "# Create the dataset on the server\n",
        "dataset.create()\n",
        "\n",
        "print(f\"New dataset '{dataset_name}' created in workspace '{workspace}'\")"
      ],
      "metadata": {
        "id": "RtG6j-RLLywU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Set the model name\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "# Set the directory where you want to save the model\n",
        "local_model_path = \"/content/tinyllama-1.1b-chat\"\n",
        "\n",
        "# Download the model\n",
        "print(f\"Downloading {model_name} to {local_model_path}...\")\n",
        "snapshot_download(repo_id=model_name, local_dir=local_model_path)\n",
        "\n",
        "# Load the tokenizer and model to verify the download\n",
        "print(\"Loading the model to verify the download...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(local_model_path)\n",
        "\n",
        "print(f\"Model {model_name} has been successfully downloaded and loaded.\")\n",
        "\n",
        "# Print the size of the downloaded model\n",
        "total_size = sum(f.stat().st_size for f in Path(local_model_path).glob('**/*') if f.is_file())\n",
        "print(f\"Total size of the downloaded model: {total_size / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "id": "keEY68YMZ-BH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the dataset to pick the highest quality responses\n",
        "filtered_dataset = load_dataset(\"DIBT/10k_prompts_ranked\", split=\"train\").filter(\n",
        "    lambda r: float(r[\"avg_rating\"]) >= 4 and int(r[\"num_responses\"]) >= 2\n",
        ")"
      ],
      "metadata": {
        "id": "nk3oEjP5N3Dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the features of your filtered dataset\n",
        "filtered_dataset.features"
      ],
      "metadata": {
        "id": "iRvWeEoCOib9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option A: Use local LLMs to create your pipelines"
      ],
      "metadata": {
        "id": "eJBZDrdetO2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from distilabel.llms import TransformersLLM\n",
        "from distilabel.pipeline import Pipeline\n",
        "from distilabel.steps import (\n",
        "    LoadDataFromDicts,\n",
        "    TextGenerationToArgilla,\n",
        ")\n",
        "from distilabel.steps.tasks import TextGeneration\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Subset your filtered dataset because of compute requirements. However, you can skip this step if you are not using a compute-constrained environment\n",
        "filtered_dataset_12 = filtered_dataset.select(range(12))\n",
        "filtered_dataset_12"
      ],
      "metadata": {
        "id": "bCKfASWsRuiR",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the pipeline\n",
        "with Pipeline(\n",
        "    name=\"prefs-with-tinyllama\",\n",
        "    description=\"Pipeline for building preference datasets using TinyLlama\",\n",
        ") as pipeline:\n",
        "    load_dataset = LoadDataFromDicts(\n",
        "        name=\"load_dataset\",\n",
        "        data=filtered_dataset_12,\n",
        "        output_mappings={\"prompt\": \"instruction\"},\n",
        "    )\n",
        "    text_generation = TextGeneration(\n",
        "        name=\"text_generation\",\n",
        "        llm=TransformersLLM(\n",
        "            model=local_model_path,\n",
        "            device_map=\"auto\",  # This will use available GPU(s) efficiently\n",
        "            torch_dtype=\"auto\",  # This will use the appropriate dtype for the model\n",
        "            trust_remote_code=True,  # This may be necessary for some models\n",
        "            model_kwargs={\n",
        "                \"low_cpu_mem_usage\": True,  # This can help with memory issues\n",
        "            },\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    to_argilla = TextGenerationToArgilla(\n",
        "        name=\"text_generation_to_argilla\",\n",
        "        dataset_name=dataset_name,\n",
        "        dataset_workspace=workspace,\n",
        "    )\n",
        "    load_dataset >> text_generation >> to_argilla\n",
        "\n",
        "# Run the pipeline\n",
        "distiset = pipeline.run(\n",
        "    parameters={\n",
        "        \"load_dataset\": {\n",
        "            \"batch_size\": 16,\n",
        "        },\n",
        "        \"text_generation\": {\n",
        "            \"llm\": {\n",
        "                \"generation_kwargs\": {\n",
        "                    \"max_new_tokens\": 512,\n",
        "                    \"temperature\": 0.7,\n",
        "                    \"do_sample\": True,\n",
        "                    \"top_p\": 0.95,\n",
        "                    \"top_k\": 50,\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        \"text_generation_to_argilla\": {\n",
        "            \"api_url\": api_url,\n",
        "            \"api_key\": api_key,\n",
        "            \"dataset_name\": dataset_name,\n",
        "            \"dataset_workspace\": workspace,\n",
        "        },\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "cEO-bu-wSemr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option B: Use OpenAI LLM to create your pipeline"
      ],
      "metadata": {
        "id": "LInKipDctVi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from distilabel.llms import OpenAILLM\n",
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "sak_7OBoSyfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the pipeline\n",
        "with Pipeline(\n",
        "    name=\"prefs-with-openai\",\n",
        "    description=\"Pipeline for building preference datasets using OpenAI\",\n",
        ") as pipeline:\n",
        "    load_dataset = LoadDataFromDicts(\n",
        "        name=\"load_dataset\",\n",
        "        data=filtered_dataset_12,\n",
        "        output_mappings={\"prompt\": \"instruction\"},\n",
        "    )\n",
        "    text_generation = TextGeneration(\n",
        "        name=\"text_generation\",\n",
        "        llm=OpenAILLM(model=\"gpt-4\")\n",
        "    )\n",
        "\n",
        "    to_argilla = TextGenerationToArgilla(\n",
        "        name=\"text_generation_to_argilla\",\n",
        "        dataset_name=dataset_name,\n",
        "        dataset_workspace=workspace,\n",
        "    )\n",
        "    load_dataset >> text_generation >> to_argilla\n",
        "\n",
        "# Run the pipeline\n",
        "distiset = pipeline.run(\n",
        "    parameters={\n",
        "        \"load_dataset\": {\n",
        "            \"batch_size\": 16,\n",
        "        },\n",
        "        \"text_generation\": {\n",
        "            \"llm\": {\n",
        "                \"generation_kwargs\": {\n",
        "                    \"temperature\": 0.7,\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        \"text_generation_to_argilla\": {\n",
        "            \"api_url\": api_url,\n",
        "            \"api_key\": api_key,\n",
        "            \"dataset_name\": dataset_name,\n",
        "            \"dataset_workspace\": workspace,\n",
        "        },\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "fb6ExefESybC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Eleuther Evaluation Harness"
      ],
      "metadata": {
        "id": "TsrpHkvIt1Jt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "git clone https://github.com/EleutherAI/lm-evaluation-harness\n",
        "cd lm-evaluation-harness\n",
        "pip install -e ."
      ],
      "metadata": {
        "id": "M4V2U_7BfgV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the LLM using Eleuther Evaluation Harness"
      ],
      "metadata": {
        "id": "oKfCxcZDuDa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "lm_eval --model hf \\\n",
        "    --model_args pretrained=EleutherAI/pythia-160m,revision=step100000,dtype=\"float\" \\\n",
        "    --tasks hellaswag \\\n",
        "    --device cuda \\\n",
        "    --batch_size auto:4 \\\n",
        "    --output_path hellaswag_test"
      ],
      "metadata": {
        "id": "3tQj162gfzdP",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eCkteYyOFyva"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}