{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0dd1482ff9dc4aae9e61b7322b71ec11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_78c41cee35004186accdb029f0d6e708",
              "IPY_MODEL_344a464889284b76831ee4706e20b376",
              "IPY_MODEL_8e9ad24006474576ac896e3d20bbaa15"
            ],
            "layout": "IPY_MODEL_de2e358e888f43c5bac4ce0b83478424"
          }
        },
        "78c41cee35004186accdb029f0d6e708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_419efd2ee98a4d17adf81a2f89f86ab3",
            "placeholder": "​",
            "style": "IPY_MODEL_2d411bce89e34764ac3d6bd053a4a75a",
            "value": "Fetching 10 files: 100%"
          }
        },
        "344a464889284b76831ee4706e20b376": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a44cf7e26a64d3888a2bed5ee2335e9",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_52098b3d1f2e415ba3fc9469e12f2cd7",
            "value": 10
          }
        },
        "8e9ad24006474576ac896e3d20bbaa15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92be81a7fdfb42658d9c16d4c1f299fe",
            "placeholder": "​",
            "style": "IPY_MODEL_19293d8e58224bfb8d4ff6677700618e",
            "value": " 10/10 [00:00&lt;00:00, 393.56it/s]"
          }
        },
        "de2e358e888f43c5bac4ce0b83478424": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "419efd2ee98a4d17adf81a2f89f86ab3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d411bce89e34764ac3d6bd053a4a75a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a44cf7e26a64d3888a2bed5ee2335e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52098b3d1f2e415ba3fc9469e12f2cd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "92be81a7fdfb42658d9c16d4c1f299fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19293d8e58224bfb8d4ff6677700618e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring LLM Dataset Creation and Evaluation\n",
        "\n",
        "## Install the required libraries"
      ],
      "metadata": {
        "id": "cNxVuXtVuc9c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MqxvscIaXsX8",
        "outputId": "eb608c28-fffa-4016-f632-4aaee8585e0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: distilabel 1.4.1 does not provide the extra 'huggingface'\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qqq huggingface-hub argilla \"distilabel[huggingface]\" accelerate openai datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import argilla as rg\n",
        "from google.colab import userdata\n",
        "\n",
        "# Authenticate with Argilla\n",
        "dataset = load_dataset(\"DIBT/10k_prompts_ranked\")\n",
        "column_names = dataset[\"train\"].column_names\n",
        "print(column_names)"
      ],
      "metadata": {
        "id": "ih2qY7rEampt",
        "outputId": "17a3fa81-3fff-4956-8b5c-1ee8b1fcd02a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['prompt', 'quality', 'metadata', 'avg_rating', 'num_responses', 'agreement_ratio', 'raw_responses', 'kind', 'cluster_description', 'topic']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][0]"
      ],
      "metadata": {
        "id": "1-1FVMiPdZrP",
        "outputId": "888f02ac-be0a-424a-9e08-b69cac2992f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prompt': 'Provide step-by-step instructions on how to make a safe and effective homemade all-purpose cleaner from common household ingredients. The guide should include measurements, tips for storing the cleaner, and additional variations or scents that can be added. Additionally, the guide should be written in clear and concise language, with helpful visuals or photographs to aid in the process.',\n",
              " 'quality': [{'user_id': 'd23b12c2-b601-490e-b5b3-2040eb393a00',\n",
              "   'value': '4',\n",
              "   'status': 'submitted'},\n",
              "  {'user_id': 'e2bdd868-f28e-46fc-9254-a6ec1e291889',\n",
              "   'value': '4',\n",
              "   'status': 'submitted'}],\n",
              " 'metadata': '{\"source\": \"ultrachat\", \"kind\": \"synthetic\", \"evolved_from\": null}',\n",
              " 'avg_rating': 5.0,\n",
              " 'num_responses': 2,\n",
              " 'agreement_ratio': 1.0,\n",
              " 'raw_responses': [5, 5],\n",
              " 'kind': 'synthetic',\n",
              " 'cluster_description': 'Sustainable Packaging & Skin Care Products',\n",
              " 'topic': 'Environmental Issues'}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argilla as rg\n",
        "\n",
        "# Initialize Argilla client\n",
        "\n",
        "# Initialize the Argilla client\n",
        "\n",
        "from uuid import uuid4\n",
        "client = rg.Argilla(\n",
        "    api_url=userdata.get('argilla_api_url'),\n",
        "    api_key=userdata.get('argilla_api_key')\n",
        ")\n",
        "\n",
        "workspace = \"argilla\"\n",
        "dataset_name = f\"DIBT_sample_prompts_{uuid4()}\"\n",
        "\n",
        "# Create a new Dataset\n",
        "dataset = rg.Dataset(\n",
        "    name=dataset_name,\n",
        "    workspace=workspace,\n",
        "    client=client\n",
        ")\n",
        "\n",
        "# Configure the dataset settings\n",
        "dataset.settings.fields = [\n",
        "    rg.TextField(name=\"id\"),\n",
        "    rg.TextField(name=\"instruction\"),\n",
        "    rg.TextField(name=\"generation\"),\n",
        "]\n",
        "\n",
        "dataset.settings.questions = [\n",
        "    rg.LabelQuestion(\n",
        "        name=\"quality\",\n",
        "        labels=[\"👎\", \"👍\"],\n",
        "        title=\"Quality of the generated text\",\n",
        "    )\n",
        "]\n",
        "\n",
        "# Create the dataset on the server\n",
        "dataset.create()\n",
        "\n",
        "print(f\"New dataset '{dataset_name}' created in workspace '{workspace}'\")"
      ],
      "metadata": {
        "id": "RtG6j-RLLywU",
        "outputId": "300deada-026a-4bd1-f3ba-c9cc01191e27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/logging/handlers.py\", line 1475, in emit\n",
            "    self.enqueue(self.prepare(record))\n",
            "  File \"/usr/lib/python3.10/logging/handlers.py\", line 1436, in enqueue\n",
            "    self.queue.put_nowait(record)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 138, in put_nowait\n",
            "    return self.put(obj, False)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 88, in put\n",
            "    raise ValueError(f\"Queue {self!r} is closed\")\n",
            "ValueError: Queue <multiprocessing.queues.Queue object at 0x7e77bc2858a0> is closed\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-20-aede63ceddf1>\", line 8, in <cell line: 8>\n",
            "    client = rg.Argilla(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/argilla/client.py\", line 71, in __init__\n",
            "    super().__init__(api_url=api_url, api_key=api_key, timeout=timeout, retries=retries, **http_client_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/argilla/_api/_client.py\", line 136, in __init__\n",
            "    self._validate_connection()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/argilla/_api/_client.py\", line 152, in _validate_connection\n",
            "    self.log(message=message, level=logging.INFO)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/argilla/_api/_client.py\", line 147, in log\n",
            "    logging.log(level=level, msg=message)\n",
            "Message: 'Argilla: Logged in as bhuvana-ak7 with the role owner'\n",
            "Arguments: ()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New dataset 'DIBT_sample_prompts_ff3c49a6-e676-4536-92cc-0e22bfc568c9' created in workspace 'argilla'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Set the model name\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "# Set the directory where you want to save the model\n",
        "local_model_path = \"/content/tinyllama-1.1b-chat\"\n",
        "\n",
        "# Download the model\n",
        "print(f\"Downloading {model_name} to {local_model_path}...\")\n",
        "snapshot_download(repo_id=model_name, local_dir=local_model_path)\n",
        "\n",
        "# Load the tokenizer and model to verify the download\n",
        "print(\"Loading the model to verify the download...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(local_model_path)\n",
        "\n",
        "print(f\"Model {model_name} has been successfully downloaded and loaded.\")\n",
        "\n",
        "# Print the size of the downloaded model\n",
        "total_size = sum(f.stat().st_size for f in Path(local_model_path).glob('**/*') if f.is_file())\n",
        "print(f\"Total size of the downloaded model: {total_size / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "id": "keEY68YMZ-BH",
        "outputId": "d879ce47-911e-49e5-f3c6-710a4e04e0bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "0dd1482ff9dc4aae9e61b7322b71ec11",
            "78c41cee35004186accdb029f0d6e708",
            "344a464889284b76831ee4706e20b376",
            "8e9ad24006474576ac896e3d20bbaa15",
            "de2e358e888f43c5bac4ce0b83478424",
            "419efd2ee98a4d17adf81a2f89f86ab3",
            "2d411bce89e34764ac3d6bd053a4a75a",
            "7a44cf7e26a64d3888a2bed5ee2335e9",
            "52098b3d1f2e415ba3fc9469e12f2cd7",
            "92be81a7fdfb42658d9c16d4c1f299fe",
            "19293d8e58224bfb8d4ff6677700618e"
          ]
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading TinyLlama/TinyLlama-1.1B-Chat-v1.0 to /content/tinyllama-1.1b-chat...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0dd1482ff9dc4aae9e61b7322b71ec11"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the model to verify the download...\n",
            "Model TinyLlama/TinyLlama-1.1B-Chat-v1.0 has been successfully downloaded and loaded.\n",
            "Total size of the downloaded model: 2.20 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the dataset to pick the highest quality responses\n",
        "filtered_dataset = load_dataset(\"DIBT/10k_prompts_ranked\", split=\"train\").filter(\n",
        "    lambda r: float(r[\"avg_rating\"]) >= 4 and int(r[\"num_responses\"]) >= 2\n",
        ")"
      ],
      "metadata": {
        "id": "nk3oEjP5N3Dg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the features of your filtered dataset\n",
        "filtered_dataset.features"
      ],
      "metadata": {
        "id": "iRvWeEoCOib9",
        "outputId": "8971a260-f634-4efc-aecd-c99474eef8f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prompt': Value(dtype='string', id='field'),\n",
              " 'quality': [{'user_id': Value(dtype='string', id='question'),\n",
              "   'value': Value(dtype='string', id='suggestion'),\n",
              "   'status': Value(dtype='string', id='question')}],\n",
              " 'metadata': Value(dtype='string', id='metadata'),\n",
              " 'avg_rating': Value(dtype='float64', id=None),\n",
              " 'num_responses': Value(dtype='int64', id=None),\n",
              " 'agreement_ratio': Value(dtype='float64', id=None),\n",
              " 'raw_responses': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
              " 'kind': Value(dtype='string', id=None),\n",
              " 'cluster_description': Value(dtype='string', id=None),\n",
              " 'topic': Value(dtype='string', id=None)}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option A: Use local LLMs to create your pipelines"
      ],
      "metadata": {
        "id": "eJBZDrdetO2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from distilabel.llms import TransformersLLM\n",
        "from distilabel.pipeline import Pipeline\n",
        "from distilabel.steps import (\n",
        "    LoadDataFromDicts,\n",
        "    TextGenerationToArgilla,\n",
        ")\n",
        "from distilabel.steps.tasks import TextGeneration\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Subset your filtered dataset because of compute requirements. However, you can skip this step if you are not using a compute-constrained environment\n",
        "filtered_dataset_10 = filtered_dataset.select(range(10))\n",
        "filtered_dataset_10"
      ],
      "metadata": {
        "id": "bCKfASWsRuiR",
        "collapsed": true,
        "outputId": "884bbb19-259b-45c5-a4bc-d6ff52518866",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['prompt', 'quality', 'metadata', 'avg_rating', 'num_responses', 'agreement_ratio', 'raw_responses', 'kind', 'cluster_description', 'topic'],\n",
              "    num_rows: 10\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the pipeline\n",
        "with Pipeline(\n",
        "    name=\"prefs-with-tinyllama\",\n",
        "    description=\"Pipeline for building preference datasets using TinyLlama\",\n",
        ") as pipeline:\n",
        "    load_dataset = LoadDataFromDicts(\n",
        "        name=\"load_dataset\",\n",
        "        data=filtered_dataset_1,\n",
        "        output_mappings={\"prompt\": \"instruction\"},\n",
        "    )\n",
        "    text_generation = TextGeneration(\n",
        "        name=\"text_generation\",\n",
        "        llm=TransformersLLM(\n",
        "            model=local_model_path,\n",
        "            device_map=\"auto\",  # This will use available GPU(s) efficiently\n",
        "            torch_dtype=\"auto\",  # This will use the appropriate dtype for the model\n",
        "            trust_remote_code=True,  # This may be necessary for some models\n",
        "            model_kwargs={\n",
        "                \"low_cpu_mem_usage\": True,  # This can help with memory issues\n",
        "            },\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    to_argilla = TextGenerationToArgilla(\n",
        "        name=\"text_generation_to_argilla\",\n",
        "        dataset_name=dataset_name,\n",
        "        dataset_workspace=workspace,\n",
        "    )\n",
        "    load_dataset >> text_generation >> to_argilla\n",
        "\n",
        "# Run the pipeline\n",
        "distiset = pipeline.run(\n",
        "    parameters={\n",
        "        \"load_dataset\": {\n",
        "            \"batch_size\": 5,\n",
        "        },\n",
        "        \"text_generation\": {\n",
        "            \"llm\": {\n",
        "                \"generation_kwargs\": {\n",
        "                    \"max_new_tokens\": 512,\n",
        "                    \"temperature\": 0.7,\n",
        "                    \"do_sample\": True,\n",
        "                    \"top_p\": 0.95,\n",
        "                    \"top_k\": 50,\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        \"text_generation_to_argilla\": {\n",
        "            \"api_url\": userdata.get('argilla_api_url'),\n",
        "            \"api_key\": userdata.get('argilla_api_key'),\n",
        "            \"dataset_name\": dataset_name,\n",
        "            \"dataset_workspace\": workspace,\n",
        "        },\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "cEO-bu-wSemr",
        "outputId": "a9489af4-3dc6-4cf6-fb95-ca604f0c6a27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[10/20/24 16:16:40]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m\u001b[32m'distilabel.pipeline'\u001b[0m\u001b[1m]\u001b[0m 💾 Loading `_BatchManager` from cache:             \u001b]8;id=976601;file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py\u001b\\\u001b[2mbase.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=670108;file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py#818\u001b\\\u001b[2m818\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[32m'/root/.cache/distilabel/pipelines/prefs-with-tinyllama/1a440b4593dcf2dafb\u001b[0m \u001b[2m           \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[32mc5a3a48b5ca698109ed0e1/executions/43fffc84c5b1b66aca994ae41327e022af27a110\u001b[0m \u001b[2m           \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[32m/batch_manager.json'\u001b[0m                                                       \u001b[2m           \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[10/20/24 16:16:40] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'distilabel.pipeline'</span><span style=\"font-weight: bold\">]</span> 💾 Loading `_BatchManager` from cache:             <a href=\"file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py#818\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">818</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">'/root/.cache/distilabel/pipelines/prefs-with-tinyllama/1a440b4593dcf2dafb</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">c5a3a48b5ca698109ed0e1/executions/43fffc84c5b1b66aca994ae41327e022af27a110</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">/batch_manager.json'</span>                                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m\u001b[32m'distilabel.pipeline'\u001b[0m\u001b[1m]\u001b[0m 💾 Loaded batch manager from cache doesn't contain \u001b]8;id=371998;file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py\u001b\\\u001b[2mbase.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=308128;file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py#373\u001b\\\u001b[2m373\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         any remaining data. Returning `Distiset` from cache data\u001b[33m...\u001b[0m                \u001b[2m           \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'distilabel.pipeline'</span><span style=\"font-weight: bold\">]</span> 💾 Loaded batch manager from cache doesn't contain <a href=\"file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py#373\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">373</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         any remaining data. Returning `Distiset` from cache data<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option B: Use OpenAI LLM to create your pipeline"
      ],
      "metadata": {
        "id": "LInKipDctVi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from distilabel.llms import OpenAILLM\n",
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "sak_7OBoSyfJ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_dataset_10.features\n",
        "#dataset_name"
      ],
      "metadata": {
        "id": "k3Td9N5XzO84",
        "outputId": "9acba4d9-8cc5-4d1e-cab4-9475d325b573",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prompt': Value(dtype='string', id='field'),\n",
              " 'quality': [{'user_id': Value(dtype='string', id='question'),\n",
              "   'value': Value(dtype='string', id='suggestion'),\n",
              "   'status': Value(dtype='string', id='question')}],\n",
              " 'metadata': Value(dtype='string', id='metadata'),\n",
              " 'avg_rating': Value(dtype='float64', id=None),\n",
              " 'num_responses': Value(dtype='int64', id=None),\n",
              " 'agreement_ratio': Value(dtype='float64', id=None),\n",
              " 'raw_responses': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
              " 'kind': Value(dtype='string', id=None),\n",
              " 'cluster_description': Value(dtype='string', id=None),\n",
              " 'topic': Value(dtype='string', id=None)}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the pipeline\n",
        "with Pipeline(\n",
        "    name=\"prefs-with-openai\",\n",
        "    description=\"Pipeline for building preference datasets using OpenAI\",\n",
        ") as pipeline:\n",
        "    load_dataset = LoadDataFromDicts(\n",
        "        name=\"load_dataset\",\n",
        "        data=filtered_dataset_10,\n",
        "        output_mappings={\"prompt\": \"instruction\"},\n",
        "    )\n",
        "    text_generation = TextGeneration(\n",
        "        name=\"text_generation\",\n",
        "        llm=OpenAILLM(model=\"gpt-4\")\n",
        "    )\n",
        "\n",
        "    to_argilla = TextGenerationToArgilla(\n",
        "        name=\"text_generation_to_argilla\",\n",
        "        dataset_name=dataset_name,\n",
        "        dataset_workspace=workspace,\n",
        "    )\n",
        "    load_dataset >> text_generation >> to_argilla\n",
        "\n",
        "# Run the pipeline\n",
        "distiset = pipeline.run(\n",
        "    parameters={\n",
        "        \"load_dataset\": {\n",
        "            \"batch_size\": 16,\n",
        "        },\n",
        "        \"text_generation\": {\n",
        "            \"llm\": {\n",
        "                \"generation_kwargs\": {\n",
        "                    \"temperature\": 0.7,\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        \"text_generation_to_argilla\": {\n",
        "            \"api_url\": userdata.get('argilla_api_url'),\n",
        "            \"api_key\": userdata.get('argilla_api_key'),\n",
        "            \"dataset_name\": dataset_name,\n",
        "            \"dataset_workspace\": workspace,\n",
        "        },\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "fb6ExefESybC",
        "outputId": "adeac6c4-733a-483a-ee79-1c8b16c2a458",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[10/20/24 16:41:51]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m\u001b[32m'distilabel.pipeline'\u001b[0m\u001b[1m]\u001b[0m 📝 Pipeline data will be written to                \u001b]8;id=995473;file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py\u001b\\\u001b[2mbase.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=370283;file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py#866\u001b\\\u001b[2m866\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[32m'/root/.cache/distilabel/pipelines/prefs-with-openai/1a440b4593dcf2dafbc5a\u001b[0m \u001b[2m           \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[32m3a48b5ca698109ed0e1/executions/68b81c16f5090128c911e0a36c459bcae24f4a9a/da\u001b[0m \u001b[2m           \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[32mta/steps_outputs'\u001b[0m                                                          \u001b[2m           \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[10/20/24 16:41:51] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'distilabel.pipeline'</span><span style=\"font-weight: bold\">]</span> 📝 Pipeline data will be written to                <a href=\"file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py#866\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">866</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">'/root/.cache/distilabel/pipelines/prefs-with-openai/1a440b4593dcf2dafbc5a</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">3a48b5ca698109ed0e1/executions/68b81c16f5090128c911e0a36c459bcae24f4a9a/da</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">ta/steps_outputs'</span>                                                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m\u001b[32m'distilabel.pipeline'\u001b[0m\u001b[1m]\u001b[0m ⌛ The steps of the pipeline will be loaded in     \u001b]8;id=883457;file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py\u001b\\\u001b[2mbase.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=19541;file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py#889\u001b\\\u001b[2m889\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         stages:                                                                    \u001b[2m           \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m          * Stage \u001b[1;36m0\u001b[0m:                                                                \u001b[2m           \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m            - \u001b[32m'load_dataset'\u001b[0m \u001b[1m(\u001b[0mresults cached, won't be loaded and executed\u001b[1m)\u001b[0m         \u001b[2m           \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m            - \u001b[32m'text_generation'\u001b[0m \u001b[1m(\u001b[0mresults cached, won't be loaded and executed\u001b[1m)\u001b[0m      \u001b[2m           \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m            - \u001b[32m'text_generation_to_argilla'\u001b[0m                                          \u001b[2m           \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'distilabel.pipeline'</span><span style=\"font-weight: bold\">]</span> ⌛ The steps of the pipeline will be loaded in     <a href=\"file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py#889\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">889</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         stages:                                                                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>          * Stage <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>:                                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>            - <span style=\"color: #008000; text-decoration-color: #008000\">'load_dataset'</span> <span style=\"font-weight: bold\">(</span>results cached, won't be loaded and executed<span style=\"font-weight: bold\">)</span>         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>            - <span style=\"color: #008000; text-decoration-color: #008000\">'text_generation'</span> <span style=\"font-weight: bold\">(</span>results cached, won't be loaded and executed<span style=\"font-weight: bold\">)</span>      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>            - <span style=\"color: #008000; text-decoration-color: #008000\">'text_generation_to_argilla'</span>                                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[10/20/24 16:41:52]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m\u001b[32m'distilabel.pipeline'\u001b[0m\u001b[1m]\u001b[0m ⏳ Waiting for all the steps of stage \u001b[1;36m0\u001b[0m to        \u001b]8;id=516216;file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py\u001b\\\u001b[2mbase.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=944834;file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py#1183\u001b\\\u001b[2m1183\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         load\u001b[33m...\u001b[0m                                                                   \u001b[2m            \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[10/20/24 16:41:52] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'distilabel.pipeline'</span><span style=\"font-weight: bold\">]</span> ⏳ Waiting for all the steps of stage <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> to        <a href=\"file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py#1183\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1183</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         load<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m\u001b[32m'root'\u001b[0m\u001b[1m]\u001b[0m Argilla: Logged in as bhuvana-ak7 with the role owner          \u001b]8;id=561798;file:///usr/local/lib/python3.10/dist-packages/argilla/_api/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=486596;file:///usr/local/lib/python3.10/dist-packages/argilla/_api/_client.py#147\u001b\\\u001b[2m147\u001b[0m\u001b]8;;\u001b\\\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'root'</span><span style=\"font-weight: bold\">]</span> Argilla: Logged in as bhuvana-ak7 with the role owner          <a href=\"file:///usr/local/lib/python3.10/dist-packages/argilla/_api/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.10/dist-packages/argilla/_api/_client.py#147\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">147</span></a>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[10/20/24 16:41:54]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m\u001b[32m'distilabel.pipeline'\u001b[0m\u001b[1m]\u001b[0m ⏳ Steps from stage \u001b[1;36m0\u001b[0m loaded: \u001b[1;36m1\u001b[0m/\u001b[1;36m1\u001b[0m                 \u001b]8;id=261272;file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py\u001b\\\u001b[2mbase.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=942813;file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py#1216\u001b\\\u001b[2m1216\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m          * \u001b[32m'text_generation_to_argilla'\u001b[0m replicas: \u001b[1;36m1\u001b[0m/\u001b[1;36m1\u001b[0m                             \u001b[2m            \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[10/20/24 16:41:54] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'distilabel.pipeline'</span><span style=\"font-weight: bold\">]</span> ⏳ Steps from stage <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> loaded: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>                 <a href=\"file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py#1216\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1216</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>          * <span style=\"color: #008000; text-decoration-color: #008000\">'text_generation_to_argilla'</span> replicas: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m\u001b[32m'distilabel.pipeline'\u001b[0m\u001b[1m]\u001b[0m ✅ All the steps from stage \u001b[1;36m0\u001b[0m have been loaded!   \u001b]8;id=70017;file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py\u001b\\\u001b[2mbase.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=826157;file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py#1220\u001b\\\u001b[2m1220\u001b[0m\u001b]8;;\u001b\\\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'distilabel.pipeline'</span><span style=\"font-weight: bold\">]</span> ✅ All the steps from stage <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> have been loaded!   <a href=\"file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py#1220\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1220</span></a>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m\u001b[32m'distilabel.step.text_generation_to_argilla'\u001b[0m\u001b[1m]\u001b[0m 📦 Processing batch \u001b]8;id=339322;file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/step_wrapper.py\u001b\\\u001b[2mstep_wrapper.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=969569;file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/step_wrapper.py#229\u001b\\\u001b[2m229\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1;36m0\u001b[0m in \u001b[32m'text_generation_to_argilla'\u001b[0m \u001b[1m(\u001b[0mreplica ID: \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m                  \u001b[2m                   \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'distilabel.step.text_generation_to_argilla'</span><span style=\"font-weight: bold\">]</span> 📦 Processing batch <a href=\"file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/step_wrapper.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">step_wrapper.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/step_wrapper.py#229\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">229</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> in <span style=\"color: #008000; text-decoration-color: #008000\">'text_generation_to_argilla'</span> <span style=\"font-weight: bold\">(</span>replica ID: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[10/20/24 16:56:30]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m\u001b[32m'distilabel.pipeline'\u001b[0m\u001b[1m]\u001b[0m 🛑 Stopping pipeline. Waiting for steps to finish \u001b]8;id=500504;file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/local.py\u001b\\\u001b[2mlocal.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=62644;file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/local.py#419\u001b\\\u001b[2m419\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         processing batches\u001b[33m...\u001b[0m                                                     \u001b[2m            \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[10/20/24 16:56:30] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'distilabel.pipeline'</span><span style=\"font-weight: bold\">]</span> 🛑 Stopping pipeline. Waiting for steps to finish <a href=\"file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/local.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">local.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/local.py#419\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">419</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         processing batches<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[10/20/24 16:56:31]\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m \u001b[1m[\u001b[0m\u001b[32m'distilabel.pipeline'\u001b[0m\u001b[1m]\u001b[0m 🛑 Press again to force the pipeline to stop.     \u001b]8;id=639607;file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/local.py\u001b\\\u001b[2mlocal.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=159060;file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/local.py#392\u001b\\\u001b[2m392\u001b[0m\u001b]8;;\u001b\\\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[10/20/24 16:56:31] </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'distilabel.pipeline'</span><span style=\"font-weight: bold\">]</span> 🛑 Press again to force the pipeline to stop.     <a href=\"file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/local.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">local.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/local.py#392\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">392</span></a>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Eleuther Evaluation Harness"
      ],
      "metadata": {
        "id": "TsrpHkvIt1Jt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "git clone https://github.com/EleutherAI/lm-evaluation-harness\n",
        "cd lm-evaluation-harness\n",
        "pip install -e ."
      ],
      "metadata": {
        "id": "M4V2U_7BfgV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the LLM using Eleuther Evaluation Harness"
      ],
      "metadata": {
        "id": "oKfCxcZDuDa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "lm_eval --model hf \\\n",
        "    --model_args pretrained=EleutherAI/pythia-160m,revision=step100000,dtype=\"float\" \\\n",
        "    --tasks hellaswag \\\n",
        "    --device cuda \\\n",
        "    --batch_size auto:4 \\\n",
        "    --output_path hellaswag_test"
      ],
      "metadata": {
        "id": "3tQj162gfzdP",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eCkteYyOFyva"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}