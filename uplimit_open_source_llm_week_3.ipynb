{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2cf7bf837dfe4f649a7d9c49066bf4db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_40f074c51adb4be0a3b710bb568945e7",
              "IPY_MODEL_01317ed07ace452e8c524dd3a117d176",
              "IPY_MODEL_f17c0efedd4a4b95baeef4d3006196c4"
            ],
            "layout": "IPY_MODEL_b0e622f9db7a4784905a58078c0b9037"
          }
        },
        "40f074c51adb4be0a3b710bb568945e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b4426eb0cad4d91be45fb967e19f3ef",
            "placeholder": "​",
            "style": "IPY_MODEL_1df7c02981c7481ea0aa97e897259f0c",
            "value": "ggml-model-Q4_K_M.gguf: 100%"
          }
        },
        "01317ed07ace452e8c524dd3a117d176": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_349ccbfb1d7b4d6c9c510429ccc3a4c4",
            "max": 807693728,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_534c3845d3f74e27bed967456f25e8ae",
            "value": 807693728
          }
        },
        "f17c0efedd4a4b95baeef4d3006196c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03b2a93eb5bb434f8c183ec953a00dbd",
            "placeholder": "​",
            "style": "IPY_MODEL_693805b60748439e90da3f9f1a07b61b",
            "value": " 808M/808M [00:34&lt;00:00, 25.7MB/s]"
          }
        },
        "b0e622f9db7a4784905a58078c0b9037": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b4426eb0cad4d91be45fb967e19f3ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1df7c02981c7481ea0aa97e897259f0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "349ccbfb1d7b4d6c9c510429ccc3a4c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "534c3845d3f74e27bed967456f25e8ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "03b2a93eb5bb434f8c183ec953a00dbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "693805b60748439e90da3f9f1a07b61b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85f4cbc3e9a34935bfc925d644d4d439": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9b6582ebf2504638a0422988c7cfce27",
              "IPY_MODEL_0c285e47410d4130840a538eb1de4cea",
              "IPY_MODEL_05940046488a4fea8e1460f4756be67c"
            ],
            "layout": "IPY_MODEL_6a545aed8635413497d581ba3afceb69"
          }
        },
        "9b6582ebf2504638a0422988c7cfce27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9973e2faa704401abca7a9fd90adaf99",
            "placeholder": "​",
            "style": "IPY_MODEL_7db1143678b1490c950c17018134e56f",
            "value": "adapter_model.safetensors: 100%"
          }
        },
        "0c285e47410d4130840a538eb1de4cea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2b89e0438704c4db3375d5c34de53f0",
            "max": 45117080,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4bf42bfb197249f3b91db5da57835893",
            "value": 45117080
          }
        },
        "05940046488a4fea8e1460f4756be67c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c6c50c4958845b999e092175aac026d",
            "placeholder": "​",
            "style": "IPY_MODEL_305333d8a8544305ac50b231c55e37a7",
            "value": " 45.1M/45.1M [00:01&lt;00:00, 33.5MB/s]"
          }
        },
        "6a545aed8635413497d581ba3afceb69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9973e2faa704401abca7a9fd90adaf99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7db1143678b1490c950c17018134e56f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2b89e0438704c4db3375d5c34de53f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bf42bfb197249f3b91db5da57835893": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6c6c50c4958845b999e092175aac026d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "305333d8a8544305ac50b231c55e37a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5564b927dcd94305884338cf69403cc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1a536d603b0d4c9884e773bd6c315ae9",
              "IPY_MODEL_e9b26158db0745389149f696f11530b0",
              "IPY_MODEL_6841078a97c74deab6e5bc77821f9ca9"
            ],
            "layout": "IPY_MODEL_31bb1674034641faa750686f63b70dfd"
          }
        },
        "1a536d603b0d4c9884e773bd6c315ae9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b88f66e6da91480ba6cbca44c4242fda",
            "placeholder": "​",
            "style": "IPY_MODEL_cbfc5ade5fee43aabe1de53b72c3753b",
            "value": "tokenizer.json: 100%"
          }
        },
        "e9b26158db0745389149f696f11530b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba26934a530347febf417b761cf540c8",
            "max": 17210296,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_913c5484aa60486f9182643ea790222f",
            "value": 17210296
          }
        },
        "6841078a97c74deab6e5bc77821f9ca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82adef309b0d456a9753842c55c969d1",
            "placeholder": "​",
            "style": "IPY_MODEL_f108fc3b6deb45109524c079811ed6a6",
            "value": " 17.2M/17.2M [00:01&lt;00:00, 19.4MB/s]"
          }
        },
        "31bb1674034641faa750686f63b70dfd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b88f66e6da91480ba6cbca44c4242fda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbfc5ade5fee43aabe1de53b72c3753b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba26934a530347febf417b761cf540c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "913c5484aa60486f9182643ea790222f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82adef309b0d456a9753842c55c969d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f108fc3b6deb45109524c079811ed6a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1fdf167108e46b497b93f1b37553869": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7b0de90f9da04637a4c7ec08f18d0f48",
              "IPY_MODEL_7b4cf6cdc25e4c2382d4ef0d3091d0a8",
              "IPY_MODEL_77119c5fbed14bfc9e61646ef2d9178c"
            ],
            "layout": "IPY_MODEL_8513978404df4a6c98aa9f6abd02e343"
          }
        },
        "7b0de90f9da04637a4c7ec08f18d0f48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70e0bf648c5e486fa3fdc57ffc0041ab",
            "placeholder": "​",
            "style": "IPY_MODEL_7fdad6eb83bd48db8d890b37fe51e15c",
            "value": "Llama-3.2-1B-V1-F16.gguf: 100%"
          }
        },
        "7b4cf6cdc25e4c2382d4ef0d3091d0a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc6fd94509ef47828415187cb3d70398",
            "max": 2479599744,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b4c811c095344fc694be587c74e5997d",
            "value": 2479599744
          }
        },
        "77119c5fbed14bfc9e61646ef2d9178c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_009815dcebcd4dcaa32e6e0034a84ce1",
            "placeholder": "​",
            "style": "IPY_MODEL_4d9bd1c1e5464378b94ec22b9bb09fe3",
            "value": " 2.48G/2.48G [01:28&lt;00:00, 32.8MB/s]"
          }
        },
        "8513978404df4a6c98aa9f6abd02e343": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70e0bf648c5e486fa3fdc57ffc0041ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fdad6eb83bd48db8d890b37fe51e15c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc6fd94509ef47828415187cb3d70398": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4c811c095344fc694be587c74e5997d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "009815dcebcd4dcaa32e6e0034a84ce1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d9bd1c1e5464378b94ec22b9bb09fe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55cc7f82933d40138f12068815f5cdd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f08a410cf67442b86644d9800ebd500",
              "IPY_MODEL_156b223bad114e75b7635be45175fc1a",
              "IPY_MODEL_978c3ed7a6674fecbf1030d9ccea03cd"
            ],
            "layout": "IPY_MODEL_c231f768637f4d9b91916050be8904e4"
          }
        },
        "6f08a410cf67442b86644d9800ebd500": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a26ac1416eb4bcab56fe7dc8c3360d0",
            "placeholder": "​",
            "style": "IPY_MODEL_1f0f3b72a93d4c21a180ff63a3cb8fe9",
            "value": "model.safetensors: 100%"
          }
        },
        "156b223bad114e75b7635be45175fc1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9332ebcc614146cc94f31717aeb23491",
            "max": 2471653656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0be0fe2a10d54432b0bc59b9f5e8d0bf",
            "value": 2471653656
          }
        },
        "978c3ed7a6674fecbf1030d9ccea03cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09f3b5a91a124b28ad0a348e1a6b1c8e",
            "placeholder": "​",
            "style": "IPY_MODEL_7f504aaee69642118b86acca12da2382",
            "value": " 2.47G/2.47G [01:32&lt;00:00, 25.3MB/s]"
          }
        },
        "c231f768637f4d9b91916050be8904e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a26ac1416eb4bcab56fe7dc8c3360d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f0f3b72a93d4c21a180ff63a3cb8fe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9332ebcc614146cc94f31717aeb23491": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0be0fe2a10d54432b0bc59b9f5e8d0bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "09f3b5a91a124b28ad0a348e1a6b1c8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f504aaee69642118b86acca12da2382": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhuvana-ak/uplimit_projects/blob/main/uplimit_open_source_llm_week_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Llama.cpp and associated libraries"
      ],
      "metadata": {
        "id": "aR930tNAcfRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install outlines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqI9H40DUT9c",
        "outputId": "6711c793-6334-4424-adf9-4d05d261076d",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting outlines\n",
            "  Downloading outlines-0.1.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting interegular (from outlines)\n",
            "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from outlines) (3.1.4)\n",
            "Collecting lark (from outlines)\n",
            "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from outlines) (1.6.0)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from outlines) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from outlines) (3.1.0)\n",
            "Collecting diskcache (from outlines)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from outlines) (2.9.2)\n",
            "Requirement already satisfied: referencing in /usr/local/lib/python3.10/dist-packages (from outlines) (0.35.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from outlines) (4.23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outlines) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from outlines) (4.66.6)\n",
            "Collecting datasets (from outlines)\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from outlines) (4.12.2)\n",
            "Collecting pycountry (from outlines)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting airportsdata (from outlines)\n",
            "  Downloading airportsdata-20241001-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from outlines) (2.5.0+cu121)\n",
            "Collecting outlines-core==0.1.14 (from outlines)\n",
            "  Downloading outlines_core-0.1.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->outlines) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->outlines) (2.23.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->outlines) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->outlines) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->outlines)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->outlines) (2.2.2)\n",
            "Collecting xxhash (from datasets->outlines)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->outlines)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->outlines)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->outlines) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets->outlines) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->outlines) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->outlines) (6.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outlines) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outlines) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->outlines) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outlines) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->outlines) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines) (2024.10.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines) (0.20.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->outlines) (3.4.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->outlines) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->outlines) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->outlines) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->outlines) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->outlines) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->outlines) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->outlines) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->outlines) (4.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->outlines) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->outlines) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->outlines) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->outlines) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets->outlines) (0.2.0)\n",
            "Downloading outlines-0.1.1-py3-none-any.whl (91 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.7/91.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outlines_core-0.1.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (327 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.1/327.1 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading airportsdata-20241001-py3-none-any.whl (912 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m912.7/912.7 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
            "Downloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m120.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, pycountry, lark, interegular, fsspec, diskcache, dill, airportsdata, multiprocess, outlines-core, datasets, outlines\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed airportsdata-20241001 datasets-3.1.0 dill-0.3.8 diskcache-5.6.3 fsspec-2024.9.0 interegular-0.3.3 lark-1.2.2 multiprocess-0.70.16 outlines-0.1.1 outlines-core-0.1.14 pycountry-24.6.1 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "%cd llama.cpp\n",
        "!git pull\n",
        "!mkdir -p build\n",
        "%cd build\n",
        "!cmake .. -DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\n",
        "!cmake --build . --config Release\n",
        "%cd ..\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "rF_HHr9SakiS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9392f98-2606-440f-c1de-120aedc8f4ec",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 36330, done.\u001b[K\n",
            "remote: Counting objects: 100% (145/145), done.\u001b[K\n",
            "remote: Compressing objects: 100% (105/105), done.\u001b[K\n",
            "remote: Total 36330 (delta 61), reused 96 (delta 38), pack-reused 36185 (from 1)\u001b[K\n",
            "Receiving objects: 100% (36330/36330), 60.75 MiB | 16.98 MiB/s, done.\n",
            "Resolving deltas: 100% (26373/26373), done.\n",
            "/content/llama.cpp\n",
            "Already up to date.\n",
            "/content/llama.cpp/build\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP: TRUE (found version \"4.5\")\n",
            "-- OpenMP found\n",
            "-- Using llamafile\n",
            "-- Using AMX\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- x86 detected\n",
            "-- Configuring done (1.6s)\n",
            "-- Generating done (0.2s)\n",
            "\u001b[33mCMake Warning:\n",
            "  Manually-specified variables were not used by the project:\n",
            "\n",
            "    LLAMA_BLAS\n",
            "    LLAMA_BLAS_VENDOR\n",
            "\n",
            "\u001b[0m\n",
            "-- Build files have been written to: /content/llama.cpp/build\n",
            "[  0%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml.dir/ggml.c.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml.dir/ggml-alloc.c.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml.dir/ggml-quants.c.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/llamafile/sgemm.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-amx/mmq.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-amx.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml.dir/ggml-aarch64.c.o\u001b[0m\n",
            "[  4%] \u001b[32m\u001b[1mLinking CXX shared library libggml.so\u001b[0m\n",
            "[  4%] Built target ggml\n",
            "[  5%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32m\u001b[1mLinking CXX shared library libllama.so\u001b[0m\n",
            "[  8%] Built target llama\n",
            "[  8%] \u001b[34m\u001b[1mGenerating build details from Git\u001b[0m\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "[  9%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
            "[  9%] Built target build_info\n",
            "[  9%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/train.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[ 14%] Built target common\n",
            "[ 14%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
            "[ 15%] Built target test-tokenizer-0\n",
            "[ 15%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
            "[ 16%] Built target test-tokenizer-1-bpe\n",
            "[ 16%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n",
            "[ 17%] Built target test-tokenizer-1-spm\n",
            "[ 17%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n",
            "[ 18%] Built target test-log\n",
            "[ 19%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n",
            "[ 20%] Built target test-arg-parser\n",
            "[ 21%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
            "[ 22%] Built target test-quantize-fns\n",
            "[ 22%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
            "[ 23%] Built target test-quantize-perf\n",
            "[ 24%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
            "[ 25%] Built target test-sampling\n",
            "[ 25%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
            "[ 27%] Built target test-chat-template\n",
            "[ 27%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
            "[ 28%] Built target test-grammar-parser\n",
            "[ 29%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
            "[ 30%] Built target test-llama-grammar\n",
            "[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n",
            "[ 32%] Built target test-grammar-integration\n",
            "[ 32%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grad0\u001b[0m\n",
            "[ 33%] Built target test-grad0\n",
            "[ 34%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n",
            "[ 35%] Built target test-barrier\n",
            "[ 36%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
            "[ 37%] Built target test-backend-ops\n",
            "[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
            "[ 39%] Built target test-rope\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
            "[ 41%] Built target test-model-load-cancel\n",
            "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
            "[ 42%] Built target test-autorelease\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
            "[ 44%] Built target test-json-schema-to-grammar\n",
            "[ 44%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
            "[ 45%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n",
            "[ 45%] Built target test-c\n",
            "[ 45%] \u001b[32mBuilding CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n",
            "[ 46%] Built target llama-cvector-generator\n",
            "[ 47%] \u001b[32mBuilding CXX object examples/baby-llama/CMakeFiles/llama-baby-llama.dir/baby-llama.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-baby-llama\u001b[0m\n",
            "[ 47%] Built target llama-baby-llama\n",
            "[ 47%] \u001b[32mBuilding CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n",
            "[ 48%] Built target llama-batched-bench\n",
            "[ 49%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n",
            "[ 50%] Built target llama-batched\n",
            "[ 50%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n",
            "[ 51%] Built target llama-convert-llama2c-to-ggml\n",
            "[ 52%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n",
            "[ 52%] Built target llama-embedding\n",
            "[ 53%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n",
            "[ 53%] Built target llama-eval-callback\n",
            "[ 54%] \u001b[32mBuilding CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
            "[ 54%] Built target llama-export-lora\n",
            "[ 55%] \u001b[32mBuilding CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gbnf-validator\u001b[0m\n",
            "[ 55%] Built target llama-gbnf-validator\n",
            "[ 55%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n",
            "[ 55%] Built target sha256\n",
            "[ 56%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n",
            "[ 56%] Built target xxhash\n",
            "[ 57%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n",
            "[ 57%] Built target sha1\n",
            "[ 57%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n",
            "[ 58%] Built target llama-gguf-hash\n",
            "[ 58%] \u001b[32mBuilding CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n",
            "[ 59%] Built target llama-gguf-split\n",
            "[ 60%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n",
            "[ 61%] Built target llama-gguf\n",
            "[ 61%] \u001b[32mBuilding CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gritlm\u001b[0m\n",
            "[ 62%] Built target llama-gritlm\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
            "[ 63%] Built target llama-imatrix\n",
            "[ 63%] \u001b[32mBuilding CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-infill\u001b[0m\n",
            "[ 64%] Built target llama-infill\n",
            "[ 64%] \u001b[32mBuilding CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
            "[ 65%] Built target llama-bench\n",
            "[ 65%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o\u001b[0m\n",
            "[ 66%] Built target llava\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CXX static library libllava_static.a\u001b[0m\n",
            "[ 66%] Built target llava_static\n",
            "[ 67%] \u001b[32m\u001b[1mLinking CXX shared library libllava_shared.so\u001b[0m\n",
            "[ 67%] Built target llava_shared\n",
            "[ 68%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n",
            "[ 68%] Built target llama-llava-cli\n",
            "[ 68%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n",
            "[ 69%] Built target llama-minicpmv-cli\n",
            "[ 70%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n",
            "[ 70%] Built target llama-lookahead\n",
            "[ 71%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n",
            "[ 71%] Built target llama-lookup\n",
            "[ 72%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n",
            "[ 72%] Built target llama-lookup-create\n",
            "[ 73%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n",
            "[ 74%] Built target llama-lookup-merge\n",
            "[ 74%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n",
            "[ 75%] Built target llama-lookup-stats\n",
            "[ 75%] \u001b[32mBuilding CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
            "[ 76%] Built target llama-cli\n",
            "[ 76%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n",
            "[ 77%] Built target llama-parallel\n",
            "[ 77%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n",
            "[ 78%] Built target llama-passkey\n",
            "[ 78%] \u001b[32mBuilding CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n",
            "[ 79%] Built target llama-perplexity\n",
            "[ 80%] \u001b[32mBuilding CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize-stats\u001b[0m\n",
            "[ 80%] Built target llama-quantize-stats\n",
            "[ 81%] \u001b[32mBuilding CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
            "[ 81%] Built target llama-quantize\n",
            "[ 82%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n",
            "[ 82%] Built target llama-retrieval\n",
            "[ 83%] \u001b[34m\u001b[1mGenerating theme-snowstorm.css.hpp\u001b[0m\n",
            "[ 83%] \u001b[34m\u001b[1mGenerating colorthemes.css.hpp\u001b[0m\n",
            "[ 83%] \u001b[34m\u001b[1mGenerating completion.js.hpp\u001b[0m\n",
            "[ 84%] \u001b[34m\u001b[1mGenerating index-new.html.hpp\u001b[0m\n",
            "[ 84%] \u001b[34m\u001b[1mGenerating index.html.hpp\u001b[0m\n",
            "[ 85%] \u001b[34m\u001b[1mGenerating index.js.hpp\u001b[0m\n",
            "[ 86%] \u001b[34m\u001b[1mGenerating json-schema-to-grammar.mjs.hpp\u001b[0m\n",
            "[ 86%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n",
            "[ 86%] \u001b[34m\u001b[1mGenerating prompt-formats.js.hpp\u001b[0m\n",
            "[ 87%] \u001b[34m\u001b[1mGenerating style.css.hpp\u001b[0m\n",
            "[ 88%] \u001b[34m\u001b[1mGenerating system-prompts.js.hpp\u001b[0m\n",
            "[ 88%] \u001b[34m\u001b[1mGenerating theme-beeninorder.css.hpp\u001b[0m\n",
            "[ 89%] \u001b[34m\u001b[1mGenerating theme-ketivah.css.hpp\u001b[0m\n",
            "[ 89%] \u001b[34m\u001b[1mGenerating theme-mangotango.css.hpp\u001b[0m\n",
            "[ 90%] \u001b[34m\u001b[1mGenerating theme-playground.css.hpp\u001b[0m\n",
            "[ 90%] \u001b[34m\u001b[1mGenerating theme-polarnight.css.hpp\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n",
            "[ 91%] Built target llama-server\n",
            "[ 92%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n",
            "[ 93%] Built target llama-save-load-state\n",
            "[ 94%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n",
            "[ 94%] Built target llama-simple\n",
            "[ 95%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n",
            "[ 96%] Built target llama-simple-chat\n",
            "[ 96%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n",
            "[ 97%] Built target llama-speculative\n",
            "[ 97%] \u001b[32mBuilding CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n",
            "[ 98%] Built target llama-tokenize\n",
            "[ 98%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n",
            "[ 99%] Built target llama-vdot\n",
            "[100%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n",
            "[100%] Built target llama-q8dot\n",
            "/content/llama.cpp\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu\n",
            "Requirement already satisfied: numpy~=1.26.4 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements/requirements-convert_legacy_llama.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: sentencepiece~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements/requirements-convert_legacy_llama.txt (line 2)) (0.2.0)\n",
            "Collecting transformers<5.0.0,>=4.45.1 (from -r ./requirements/requirements-convert_legacy_llama.txt (line 3))\n",
            "  Downloading transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gguf>=0.1.0 (from -r ./requirements/requirements-convert_legacy_llama.txt (line 4))\n",
            "  Downloading gguf-0.10.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting protobuf<5.0.0,>=4.21.0 (from -r ./requirements/requirements-convert_legacy_llama.txt (line 5))\n",
            "  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting torch~=2.2.1 (from -r ./requirements/requirements-convert_hf_to_gguf.txt (line 3))\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torch-2.2.2%2Bcpu-cp310-cp310-linux_x86_64.whl (186.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.8/186.8 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.45.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.45.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.24.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.45.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 3)) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.45.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 3)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.45.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.45.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 3)) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.45.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.4.5)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers<5.0.0,>=4.45.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 3))\n",
            "  Downloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.45.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2024.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch~=2.2.1->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.45.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.45.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.45.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 3)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.45.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch~=2.2.1->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.3.0)\n",
            "Downloading transformers-4.46.1-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m121.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gguf-0.10.0-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, gguf, torch, tokenizers, transformers\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.0+cu121\n",
            "    Uninstalling torch-2.5.0+cu121:\n",
            "      Successfully uninstalled torch-2.5.0+cu121\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.16.1 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.5 which is incompatible.\n",
            "torchaudio 2.5.0+cu121 requires torch==2.5.0, but you have torch 2.2.2+cpu which is incompatible.\n",
            "torchvision 0.20.0+cu121 requires torch==2.5.0, but you have torch 2.2.2+cpu which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gguf-0.10.0 protobuf-4.25.5 tokenizers-0.20.1 torch-2.2.2+cpu transformers-4.46.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clone fine-tuned model from last week's task"
      ],
      "metadata": {
        "id": "qkJSwBghcqg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure Git LFS to handle large files properly\n",
        "!git lfs install\n",
        "\n",
        "# Set a higher Git LFS buffer size for large files\n",
        "!git config --global http.postBuffer 1048576000\n",
        "\n",
        "# Clone with LFS files\n",
        "!GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/bhuvana-ak7/OrpoLlama-3.2-1B-V1\n",
        "%cd OrpoLlama-3.2-1B-V1\n",
        "!git lfs pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bua6VOPkJCel",
        "outputId": "e05deedd-eb26-44b8-fe0b-c59bb8e66c54"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated git hooks.\n",
            "Git LFS initialized.\n",
            "Cloning into 'OrpoLlama-3.2-1B-V1'...\n",
            "remote: Enumerating objects: 31, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 31 (delta 9), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (31/31), 13.85 KiB | 1.73 MiB/s, done.\n",
            "/content/llama.cpp/OrpoLlama-3.2-1B-V1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/llama.cpp/OrpoLlama-3.2-1B-V1 /content/llama.cpp/models/OrpoLlama-3.2-1B-V1"
      ],
      "metadata": {
        "id": "M2vfApa_k_Fa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Llama.cpp's convert_hf_to_gguf python utility to convert your model to gguf format"
      ],
      "metadata": {
        "id": "3URsnLDucze4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/llama.cpp/convert_hf_to_gguf.py /content/llama.cpp/models/OrpoLlama-3.2-1B-V1/"
      ],
      "metadata": {
        "id": "SfVE9Q6AoDT1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e73073c-3f12-4cdc-d977-c8ce9b119538"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:hf-to-gguf:Loading model: OrpoLlama-3.2-1B-V1\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {32}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
            "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {2048, 128258}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 131072\n",
            "INFO:hf-to-gguf:gguf: embedding length = 2048\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
            "INFO:hf-to-gguf:gguf: head count = 32\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
            "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
            "INFO:hf-to-gguf:gguf: file type = 1\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:gguf.vocab:Adding 280147 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type bos to 128256\n",
            "INFO:gguf.vocab:Setting special token type eos to 128257\n",
            "INFO:gguf.vocab:Setting special token type pad to 128257\n",
            "INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
            "' + message['content'] + '<|im_end|>' + '\n",
            "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
            "' }}{% endif %}\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/llama.cpp/models/OrpoLlama-3.2-1B-V1/Llama-3.2-1B-V1-F16.gguf: n_tensors = 147, total_size = 2.5G\n",
            "Writing: 100% 2.47G/2.47G [00:05<00:00, 447Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /content/llama.cpp/models/OrpoLlama-3.2-1B-V1/Llama-3.2-1B-V1-F16.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantize your model to 4-bit\n",
        "\n",
        "The q4_k_m quantization method in llama.cpp is a 4-bit quantization format that represents key characteristics:\n",
        "\n",
        "- q4: Uses 4 bits per weight (meaning each model weight is compressed to use only 4 bits instead of the original 16/32 bits)\n",
        "- k: Uses block-wise quantization with keys/groups\n",
        "- m: Uses a multiplicative scaling factor for better accuracy (compared to additive scaling)\n",
        "\n",
        "This is generally considered a good balance between model size reduction and quality:\n",
        "\n",
        "- Reduces model size to roughly 1/8th of the original\n",
        "- Maintains reasonable quality compared to higher bit quantizations\n",
        "- Better accuracy than plain q4 due to the multiplicative scaling\n",
        "- Faster inference than 8-bit quantization methods"
      ],
      "metadata": {
        "id": "8CnSrJdZdAXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !/content/llama.cpp/build/bin/llama-quantize /content/llama.cpp/models/TinyLlama-1.1B-Chat-v1.0/TinyLlama-1.1B-Chat-v1.0-F16.gguf q4_k_m\n",
        "!/content/llama.cpp/build/bin/llama-quantize /content/llama.cpp/models/OrpoLlama-3.2-1B-V1/Llama-3.2-1B-V1-F16.gguf q4_k_m"
      ],
      "metadata": {
        "id": "8FgyLOQD6-ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ff77055-43af-403e-d090-027ef7f85d35"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 4019 (08828a6d)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/llama.cpp/models/OrpoLlama-3.2-1B-V1/Llama-3.2-1B-V1-F16.gguf' to '/content/llama.cpp/models/OrpoLlama-3.2-1B-V1/ggml-model-Q4_K_M.gguf' as Q4_K_M\n",
            "llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from /content/llama.cpp/models/OrpoLlama-3.2-1B-V1/Llama-3.2-1B-V1-F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B\n",
            "llama_model_loader: - kv   3:                            general.version str              = V1\n",
            "llama_model_loader: - kv   4:                       general.organization str              = Meta Llama\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Llama-3.2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 1B\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 16\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 64\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 64\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128258\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128258]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128258]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128256\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128257\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128257\n",
            "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
            "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   34 tensors\n",
            "llama_model_loader: - type  f16:  113 tensors\n",
            "[   1/ 147]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   2/ 147]                    rope_freqs.weight - [   32,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[   3/ 147]                    token_embd.weight - [ 2048, 128258,     1,     1], type =    f16, converting to q6_K .. size =   501.01 MiB ->   205.49 MiB\n",
            "[   4/ 147]                  blk.0.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[   5/ 147]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   6/ 147]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[   7/ 147]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[   8/ 147]                  blk.0.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
            "[   9/ 147]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  10/ 147]                blk.0.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  11/ 147]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  12/ 147]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  13/ 147]                  blk.1.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  14/ 147]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  15/ 147]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  16/ 147]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  17/ 147]                  blk.1.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
            "[  18/ 147]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  19/ 147]                blk.1.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  20/ 147]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  21/ 147]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  22/ 147]                  blk.2.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  23/ 147]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  24/ 147]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  25/ 147]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  26/ 147]                  blk.2.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  27/ 147]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  28/ 147]                blk.2.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  29/ 147]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  30/ 147]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  31/ 147]                  blk.3.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  32/ 147]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  33/ 147]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  34/ 147]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  35/ 147]                  blk.3.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  36/ 147]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  37/ 147]                blk.3.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  38/ 147]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  39/ 147]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  40/ 147]                  blk.4.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  41/ 147]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  42/ 147]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  43/ 147]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  44/ 147]                  blk.4.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
            "[  45/ 147]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  46/ 147]                blk.4.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  47/ 147]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  48/ 147]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  49/ 147]                  blk.5.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  50/ 147]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  51/ 147]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  52/ 147]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  53/ 147]                  blk.5.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  54/ 147]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  55/ 147]                blk.5.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  56/ 147]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  57/ 147]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  58/ 147]                  blk.6.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  59/ 147]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  60/ 147]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  61/ 147]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  62/ 147]                  blk.6.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  63/ 147]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  64/ 147]                blk.6.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  65/ 147]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  66/ 147]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  67/ 147]                  blk.7.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  68/ 147]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  69/ 147]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  70/ 147]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  71/ 147]                  blk.7.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
            "[  72/ 147]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  73/ 147]                blk.7.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  74/ 147]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  75/ 147]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  76/ 147]                  blk.8.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  77/ 147]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  78/ 147]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  79/ 147]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  80/ 147]                  blk.8.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  81/ 147]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  82/ 147]                blk.8.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  83/ 147]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  84/ 147]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  85/ 147]                  blk.9.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  86/ 147]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  87/ 147]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  88/ 147]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  89/ 147]                  blk.9.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  90/ 147]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  91/ 147]                blk.9.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  92/ 147]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  93/ 147]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  94/ 147]                 blk.10.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  95/ 147]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  96/ 147]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  97/ 147]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  98/ 147]                 blk.10.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
            "[  99/ 147]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 100/ 147]               blk.10.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 101/ 147]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 102/ 147]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 103/ 147]                 blk.11.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[ 104/ 147]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 105/ 147]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 106/ 147]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 107/ 147]                 blk.11.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[ 108/ 147]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 109/ 147]               blk.11.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 110/ 147]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 111/ 147]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 112/ 147]                 blk.12.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[ 113/ 147]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 114/ 147]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 115/ 147]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 116/ 147]                 blk.12.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[ 117/ 147]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 118/ 147]               blk.12.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 119/ 147]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 120/ 147]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 121/ 147]                 blk.13.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[ 122/ 147]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 123/ 147]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 124/ 147]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 125/ 147]                 blk.13.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
            "[ 126/ 147]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 127/ 147]               blk.13.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 128/ 147]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 129/ 147]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 130/ 147]                 blk.14.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[ 131/ 147]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 132/ 147]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 133/ 147]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 134/ 147]                 blk.14.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
            "[ 135/ 147]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 136/ 147]               blk.14.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 137/ 147]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 138/ 147]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 139/ 147]                 blk.15.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[ 140/ 147]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 141/ 147]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 142/ 147]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 143/ 147]                 blk.15.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
            "[ 144/ 147]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 145/ 147]               blk.15.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 146/ 147]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 147/ 147]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "llama_model_quantize_internal: model size  =  2357.27 MB\n",
            "llama_model_quantize_internal: quant size  =   762.81 MB\n",
            "\n",
            "main: quantize time = 15899.52 ms\n",
            "main:    total time = 15899.52 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try the quantized model in a downstream task"
      ],
      "metadata": {
        "id": "ximxavm7cY15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python==0.2.85"
      ],
      "metadata": {
        "id": "8-ml52Iy4G80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ef4479b-5af2-4b9c-f1c3-fe848c286217"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python==0.2.85\n",
            "  Downloading llama_cpp_python-0.2.85.tar.gz (49.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.85) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.85) (1.26.4)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.85) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.85) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.2.85) (3.0.2)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.85-cp310-cp310-linux_x86_64.whl size=2872206 sha256=de82c1fabc7e799cc0709c4d2bf444c81be6fa5ea6cda97ca52fc0dfc3ccd8dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/e8/4e/29a754f9175ef52b6481cd75e3af4de38bf6dfa9c2972f75d4\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: llama-cpp-python\n",
            "Successfully installed llama-cpp-python-0.2.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "# Load the GGUF model\n",
        "model_path = \"/content/llama.cpp/models/OrpoLlama-3.2-1B-V1/ggml-model-Q4_K_M.gguf\" #\"/content/llama.cpp/models/uonyeka-llama-3.2.Instruct/ggml-model-Q4_K_M.gguf\"\n",
        "llm = Llama(model_path=model_path, n_ctx=512, n_batch=128)"
      ],
      "metadata": {
        "id": "FG5L61iwEmM_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4771c18e-ae02-47a5-f429-72cf8206da54"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from /content/llama.cpp/models/OrpoLlama-3.2-1B-V1/ggml-model-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B\n",
            "llama_model_loader: - kv   3:                            general.version str              = V1\n",
            "llama_model_loader: - kv   4:                       general.organization str              = Meta Llama\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Llama-3.2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 1B\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 16\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 64\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 64\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128258\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128258]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128258]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128256\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128257\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128257\n",
            "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
            "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   34 tensors\n",
            "llama_model_loader: - type q4_K:   96 tensors\n",
            "llama_model_loader: - type q6_K:   17 tensors\n",
            "llm_load_vocab: special tokens cache size = 258\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128258\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 2048\n",
            "llm_load_print_meta: n_layer          = 16\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 64\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 64\n",
            "llm_load_print_meta: n_embd_head_v    = 64\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 512\n",
            "llm_load_print_meta: n_embd_v_gqa     = 512\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 8192\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = ?B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 1.24 B\n",
            "llm_load_print_meta: model size       = 762.81 MiB (5.18 BPW) \n",
            "llm_load_print_meta: general.name     = Llama 3.2 1B\n",
            "llm_load_print_meta: BOS token        = 128256 '<|im_start|>'\n",
            "llm_load_print_meta: EOS token        = 128257 '<|im_end|>'\n",
            "llm_load_print_meta: PAD token        = 128257 '<|im_end|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOT token        = 128257 '<|im_end|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: ggml ctx size =    0.07 MiB\n",
            "llm_load_tensors:        CPU buffer size =   762.81 MiB\n",
            ".......................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 128\n",
            "llama_new_context_with_model: n_ubatch   = 128\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 500000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =    16.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   16.00 MiB, K (f16):    8.00 MiB, V (f16):    8.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    63.63 MiB\n",
            "llama_new_context_with_model: graph nodes  = 518\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '128257', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.vocab_size': '128258', 'general.file_type': '15', 'llama.attention.value_length': '64', 'llama.attention.key_length': '64', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '500000.000000', 'general.architecture': 'llama', 'tokenizer.ggml.padding_token_id': '128257', 'general.basename': 'Llama-3.2', 'tokenizer.ggml.bos_token_id': '128256', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '131072', 'general.name': 'Llama 3.2 1B', 'llama.rope.dimension_count': '64', 'general.version': 'V1', 'general.organization': 'Meta Llama', 'general.type': 'model', 'general.size_label': '1B', 'llama.embedding_length': '2048', 'llama.feed_forward_length': '8192', 'llama.block_count': '16', 'llama.attention.head_count_kv': '8'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Guessed chat format: chatml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"I am a Barbie girl in \"\n",
        "output = llm(input_text, max_tokens=300, echo=True)\n",
        "\n",
        "print(output['choices'][0]['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEqSfkmUw65C",
        "outputId": "8feb2262-587f-4ac8-d2d4-f1954ded8d0f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =      79.59 ms\n",
            "llama_print_timings:      sample time =      29.94 ms /   300 runs   (    0.10 ms per token, 10020.04 tokens per second)\n",
            "llama_print_timings: prompt eval time =      35.72 ms /     2 tokens (   17.86 ms per token,    55.99 tokens per second)\n",
            "llama_print_timings:        eval time =    7759.71 ms /   299 runs   (   25.95 ms per token,    38.53 tokens per second)\n",
            "llama_print_timings:       total time =    8182.82 ms /   301 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am a Barbie girl in 2021-10-09.Question: Find the sum of the series 1 + 1/3 + 1/9 + 1/27 + 1/63 + 1/189 + … A) 0 B) 1 C) 2 D) 3 E) 4 5.1 + 1/3 + 1/9 + 1/27 + 1/63 + 1/189 =? Solution: We have to find the sum of the series as we have to add up the first n terms of the series. Sum of the first 100 natural numbers = 1 + 2 + 3 + … + 100 = 5050. A) 0 B) 1 C) 2 D) 3 E) 4\n",
            "Question: Find the sum of the series 1 + 1/3 + 1/9 + 1/27 + 1/63 + 1/189 + … A) 0 B) 1 C) 2 D) 3 E) 4 6.1 + 1/3 + 1/9 + 1/27 + 1/63 + 1/189 + 1/486 = ? Solution: We have to find the sum of the series as we have to add up the first n terms of the series. Sum of the first 100 natural numbers = 1 +\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the quantized model on other eval tools (other than EleutherAI Evaluation Harness)"
      ],
      "metadata": {
        "id": "Wxr0z2Hdezyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "git clone https://github.com/EleutherAI/lm-evaluation-harness\n",
        "cd lm-evaluation-harness\n",
        "pip install -e ."
      ],
      "metadata": {
        "id": "a54QDsnscJZM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4d53e52-92c1-43fe-8fdd-e55877742c33"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/llama.cpp/models/OrpoLlama-3.2-1B-V1/lm-evaluation-harness\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Checking if build backend supports build_editable: started\n",
            "  Checking if build backend supports build_editable: finished with status 'done'\n",
            "  Getting requirements to build editable: started\n",
            "  Getting requirements to build editable: finished with status 'done'\n",
            "  Preparing editable metadata (pyproject.toml): started\n",
            "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.5) (0.34.2)\n",
            "Collecting evaluate (from lm_eval==0.4.5)\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.5) (3.1.0)\n",
            "Collecting jsonlines (from lm_eval==0.4.5)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.5) (2.10.1)\n",
            "Requirement already satisfied: peft>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.5) (0.13.2)\n",
            "Collecting pybind11>=2.6.2 (from lm_eval==0.4.5)\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting pytablewriter (from lm_eval==0.4.5)\n",
            "  Downloading pytablewriter-1.2.0-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting rouge-score>=0.0.4 (from lm_eval==0.4.5)\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting sacrebleu>=1.5.0 (from lm_eval==0.4.5)\n",
            "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 51.8/51.8 kB 4.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.5) (1.5.2)\n",
            "Collecting sqlitedict (from lm_eval==0.4.5)\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.5) (2.2.2+cpu)\n",
            "Collecting tqdm-multiprocess (from lm_eval==0.4.5)\n",
            "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: transformers>=4.1 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.5) (4.46.1)\n",
            "Collecting zstandard (from lm_eval==0.4.5)\n",
            "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.5) (0.3.8)\n",
            "Collecting word2number (from lm_eval==0.4.5)\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.5) (10.5.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.5) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.5) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.5) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.5) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.5) (0.24.7)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.5) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.5) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.5) (17.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.5) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.5) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.5) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.5) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.5) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.5) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.5) (3.10.10)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.5) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.5) (3.8.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.5) (1.16.0)\n",
            "Collecting portalocker (from sacrebleu>=1.5.0->lm_eval==0.4.5)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.5) (2024.9.11)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.5) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.5.0->lm_eval==0.4.5)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.5) (5.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.5) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.5) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.5) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==0.4.5) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==0.4.5) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==0.4.5) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==0.4.5) (3.1.4)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.1->lm_eval==0.4.5) (0.20.1)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->lm_eval==0.4.5) (24.2.0)\n",
            "Requirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm_eval==0.4.5) (75.1.0)\n",
            "Collecting DataProperty<2,>=1.0.1 (from pytablewriter->lm_eval==0.4.5)\n",
            "  Downloading DataProperty-1.0.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm_eval==0.4.5)\n",
            "  Downloading mbstrdecoder-1.1.3-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm_eval==0.4.5)\n",
            "  Downloading pathvalidate-3.2.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm_eval==0.4.5)\n",
            "  Downloading tabledata-1.3.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm_eval==0.4.5)\n",
            "  Downloading tcolorpy-0.1.6-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.5)\n",
            "  Downloading typepy-1.3.2-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.5) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.5) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.5) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.5) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.5) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.5) (4.0.3)\n",
            "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.10/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval==0.4.5) (5.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval==0.4.5) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval==0.4.5) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval==0.4.5) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval==0.4.5) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.5) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.9 in /usr/local/lib/python3.10/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.5) (2024.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8->lm_eval==0.4.5) (3.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score>=0.0.4->lm_eval==0.4.5) (8.1.7)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->lm_eval==0.4.5) (2024.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8->lm_eval==0.4.5) (1.3.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.16.0->lm_eval==0.4.5) (0.2.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.0/84.0 kB 8.0 MB/s eta 0:00:00\n",
            "Downloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 243.3/243.3 kB 18.0 MB/s eta 0:00:00\n",
            "Downloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 104.0/104.0 kB 11.3 MB/s eta 0:00:00\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading pytablewriter-1.2.0-py3-none-any.whl (111 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 111.1/111.1 kB 11.7 MB/s eta 0:00:00\n",
            "Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
            "Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/5.4 MB 103.6 MB/s eta 0:00:00\n",
            "Downloading DataProperty-1.0.1-py3-none-any.whl (27 kB)\n",
            "Downloading mbstrdecoder-1.1.3-py3-none-any.whl (7.8 kB)\n",
            "Downloading pathvalidate-3.2.1-py3-none-any.whl (23 kB)\n",
            "Downloading tabledata-1.3.3-py3-none-any.whl (11 kB)\n",
            "Downloading tcolorpy-0.1.6-py3-none-any.whl (8.1 kB)\n",
            "Downloading typepy-1.3.2-py3-none-any.whl (31 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Building wheels for collected packages: lm_eval, rouge-score, sqlitedict, word2number\n",
            "  Building editable for lm_eval (pyproject.toml): started\n",
            "  Building editable for lm_eval (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for lm_eval: filename=lm_eval-0.4.5-0.editable-py3-none-any.whl size=19635 sha256=974bcb5f5aba459da3c33883d85f93e17013cecc4b44028e43d6055eba4bb4dc\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-t6mzbhb5/wheels/3a/66/38/b2846d1cc43718c5c90bdf6df0f0c54090249d7e104b4693b8\n",
            "  Building wheel for rouge-score (setup.py): started\n",
            "  Building wheel for rouge-score (setup.py): finished with status 'done'\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=2daa5943711a28aca953798049ae48f2eb07bec6fb5e4fb882454001c18be1d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "  Building wheel for sqlitedict (setup.py): started\n",
            "  Building wheel for sqlitedict (setup.py): finished with status 'done'\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=0695fc9a546424a0e763ca16dee15cc4fccfd233487d457432940d8e9120c2fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
            "  Building wheel for word2number (setup.py): started\n",
            "  Building wheel for word2number (setup.py): finished with status 'done'\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=2653b366ce6ff5568568eef8fe9836d47bc3c0c0cafa8b1c2eaf1e2c2e18ecd2\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/ff/26/d3cfbd971e96c5aa3737ecfced81628830d7359b55fbb8ca3b\n",
            "Successfully built lm_eval rouge-score sqlitedict word2number\n",
            "Installing collected packages: word2number, sqlitedict, zstandard, tcolorpy, pybind11, portalocker, pathvalidate, mbstrdecoder, jsonlines, colorama, typepy, tqdm-multiprocess, sacrebleu, rouge-score, DataProperty, tabledata, pytablewriter, evaluate, lm_eval\n",
            "Successfully installed DataProperty-1.0.1 colorama-0.4.6 evaluate-0.4.3 jsonlines-4.0.0 lm_eval-0.4.5 mbstrdecoder-1.1.3 pathvalidate-3.2.1 portalocker-2.10.1 pybind11-2.13.6 pytablewriter-1.2.0 rouge-score-0.1.2 sacrebleu-2.4.3 sqlitedict-2.1.0 tabledata-1.3.3 tcolorpy-0.1.6 tqdm-multiprocess-0.0.11 typepy-1.3.2 word2number-1.1 zstandard-0.23.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'lm-evaluation-harness'...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --index-url https://download.pytorch.org/whl/cu117"
      ],
      "metadata": {
        "id": "rhgTsaXZOCtB",
        "outputId": "7b1938c8-7dae-4f13-c4b6-e421fb02135d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu117\n",
            "Collecting torch==1.13.1+cu117\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torch-1.13.1%2Bcu117-cp310-cp310-linux_x86_64.whl (1801.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m992.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.14.1+cu117\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torchvision-0.14.1%2Bcu117-cp310-cp310-linux_x86_64.whl (24.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.3/24.3 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.13.1\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torchaudio-0.13.1%2Bcu117-cp310-cp310-linux_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1+cu117) (4.12.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1+cu117) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1+cu117) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1+cu117) (10.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (2024.8.30)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.2+cpu\n",
            "    Uninstalling torch-2.2.2+cpu:\n",
            "      Successfully uninstalled torch-2.2.2+cpu\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.0+cu121\n",
            "    Uninstalling torchvision-0.20.0+cu121:\n",
            "      Successfully uninstalled torchvision-0.20.0+cu121\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.5.0+cu121\n",
            "    Uninstalling torchaudio-2.5.0+cu121:\n",
            "      Successfully uninstalled torchaudio-2.5.0+cu121\n",
            "Successfully installed torch-1.13.1+cu117 torchaudio-0.13.1+cu117 torchvision-0.14.1+cu117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "lm_eval --model hf \\\n",
        "    --model_args pretrained=bhuvana-ak7/OrpoLlama-3.2-1B-V1_q4_k_m,dtype=\"float\"\\\n",
        "    --tasks hellaswag \\\n",
        "    --device cuda \\\n",
        "    --batch_size auto:4 \\\n",
        "    --output_path hellaswag_test"
      ],
      "metadata": {
        "id": "UYx6XYJNsCEW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00763e2e-7005-42d0-a7b7-9ca4f42af5c4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
            "Determined largest batch size: 64\n",
            "Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
            "Determined largest batch size: 64\n",
            "hf (pretrained=bhuvana-ak7/OrpoLlama-3.2-1B-V1_q4_k_m,dtype=float), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto:4 (64,64,64,64,64)\n",
            "|  Tasks  |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\n",
            "|---------|------:|------|-----:|--------|---|-----:|---|-----:|\n",
            "|hellaswag|      1|none  |     0|acc     |↑  |0.4772|±  |0.0050|\n",
            "|         |       |none  |     0|acc_norm|↑  |0.6366|±  |0.0048|\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-11-03 19:06:43.167494: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-11-03 19:06:43.187248: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-03 19:06:43.210090: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-03 19:06:43.217143: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-03 19:06:43.232830: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-03 19:06:44.431881: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-11-03:19:06:46,211 INFO     [__main__.py:279] Verbosity set to INFO\n",
            "2024-11-03:19:06:56,231 INFO     [__main__.py:376] Selected Tasks: ['hellaswag']\n",
            "2024-11-03:19:06:56,232 INFO     [evaluator.py:164] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "2024-11-03:19:06:56,232 INFO     [evaluator.py:201] Initializing hf model, with arguments: {'pretrained': 'bhuvana-ak7/OrpoLlama-3.2-1B-V1_q4_k_m', 'dtype': 'float'}\n",
            "2024-11-03:19:06:56,294 INFO     [huggingface.py:130] Using device 'cuda'\n",
            "2024-11-03:19:06:56,543 INFO     [huggingface.py:483] Using model type 'default'\n",
            "2024-11-03:19:06:57,473 INFO     [huggingface.py:367] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda'}\n",
            "Loading adapter weights from bhuvana-ak7/OrpoLlama-3.2-1B-V1_q4_k_m led to unexpected keys not found in the model:  ['layers.0.mlp.down_proj.lora_A.default.weight', 'layers.0.mlp.down_proj.lora_B.default.weight', 'layers.0.mlp.gate_proj.lora_A.default.weight', 'layers.0.mlp.gate_proj.lora_B.default.weight', 'layers.0.mlp.up_proj.lora_A.default.weight', 'layers.0.mlp.up_proj.lora_B.default.weight', 'layers.0.self_attn.k_proj.lora_A.default.weight', 'layers.0.self_attn.k_proj.lora_B.default.weight', 'layers.0.self_attn.o_proj.lora_A.default.weight', 'layers.0.self_attn.o_proj.lora_B.default.weight', 'layers.0.self_attn.q_proj.lora_A.default.weight', 'layers.0.self_attn.q_proj.lora_B.default.weight', 'layers.0.self_attn.v_proj.lora_A.default.weight', 'layers.0.self_attn.v_proj.lora_B.default.weight', 'layers.1.mlp.down_proj.lora_A.default.weight', 'layers.1.mlp.down_proj.lora_B.default.weight', 'layers.1.mlp.gate_proj.lora_A.default.weight', 'layers.1.mlp.gate_proj.lora_B.default.weight', 'layers.1.mlp.up_proj.lora_A.default.weight', 'layers.1.mlp.up_proj.lora_B.default.weight', 'layers.1.self_attn.k_proj.lora_A.default.weight', 'layers.1.self_attn.k_proj.lora_B.default.weight', 'layers.1.self_attn.o_proj.lora_A.default.weight', 'layers.1.self_attn.o_proj.lora_B.default.weight', 'layers.1.self_attn.q_proj.lora_A.default.weight', 'layers.1.self_attn.q_proj.lora_B.default.weight', 'layers.1.self_attn.v_proj.lora_A.default.weight', 'layers.1.self_attn.v_proj.lora_B.default.weight', 'layers.10.mlp.down_proj.lora_A.default.weight', 'layers.10.mlp.down_proj.lora_B.default.weight', 'layers.10.mlp.gate_proj.lora_A.default.weight', 'layers.10.mlp.gate_proj.lora_B.default.weight', 'layers.10.mlp.up_proj.lora_A.default.weight', 'layers.10.mlp.up_proj.lora_B.default.weight', 'layers.10.self_attn.k_proj.lora_A.default.weight', 'layers.10.self_attn.k_proj.lora_B.default.weight', 'layers.10.self_attn.o_proj.lora_A.default.weight', 'layers.10.self_attn.o_proj.lora_B.default.weight', 'layers.10.self_attn.q_proj.lora_A.default.weight', 'layers.10.self_attn.q_proj.lora_B.default.weight', 'layers.10.self_attn.v_proj.lora_A.default.weight', 'layers.10.self_attn.v_proj.lora_B.default.weight', 'layers.11.mlp.down_proj.lora_A.default.weight', 'layers.11.mlp.down_proj.lora_B.default.weight', 'layers.11.mlp.gate_proj.lora_A.default.weight', 'layers.11.mlp.gate_proj.lora_B.default.weight', 'layers.11.mlp.up_proj.lora_A.default.weight', 'layers.11.mlp.up_proj.lora_B.default.weight', 'layers.11.self_attn.k_proj.lora_A.default.weight', 'layers.11.self_attn.k_proj.lora_B.default.weight', 'layers.11.self_attn.o_proj.lora_A.default.weight', 'layers.11.self_attn.o_proj.lora_B.default.weight', 'layers.11.self_attn.q_proj.lora_A.default.weight', 'layers.11.self_attn.q_proj.lora_B.default.weight', 'layers.11.self_attn.v_proj.lora_A.default.weight', 'layers.11.self_attn.v_proj.lora_B.default.weight', 'layers.12.mlp.down_proj.lora_A.default.weight', 'layers.12.mlp.down_proj.lora_B.default.weight', 'layers.12.mlp.gate_proj.lora_A.default.weight', 'layers.12.mlp.gate_proj.lora_B.default.weight', 'layers.12.mlp.up_proj.lora_A.default.weight', 'layers.12.mlp.up_proj.lora_B.default.weight', 'layers.12.self_attn.k_proj.lora_A.default.weight', 'layers.12.self_attn.k_proj.lora_B.default.weight', 'layers.12.self_attn.o_proj.lora_A.default.weight', 'layers.12.self_attn.o_proj.lora_B.default.weight', 'layers.12.self_attn.q_proj.lora_A.default.weight', 'layers.12.self_attn.q_proj.lora_B.default.weight', 'layers.12.self_attn.v_proj.lora_A.default.weight', 'layers.12.self_attn.v_proj.lora_B.default.weight', 'layers.13.mlp.down_proj.lora_A.default.weight', 'layers.13.mlp.down_proj.lora_B.default.weight', 'layers.13.mlp.gate_proj.lora_A.default.weight', 'layers.13.mlp.gate_proj.lora_B.default.weight', 'layers.13.mlp.up_proj.lora_A.default.weight', 'layers.13.mlp.up_proj.lora_B.default.weight', 'layers.13.self_attn.k_proj.lora_A.default.weight', 'layers.13.self_attn.k_proj.lora_B.default.weight', 'layers.13.self_attn.o_proj.lora_A.default.weight', 'layers.13.self_attn.o_proj.lora_B.default.weight', 'layers.13.self_attn.q_proj.lora_A.default.weight', 'layers.13.self_attn.q_proj.lora_B.default.weight', 'layers.13.self_attn.v_proj.lora_A.default.weight', 'layers.13.self_attn.v_proj.lora_B.default.weight', 'layers.14.mlp.down_proj.lora_A.default.weight', 'layers.14.mlp.down_proj.lora_B.default.weight', 'layers.14.mlp.gate_proj.lora_A.default.weight', 'layers.14.mlp.gate_proj.lora_B.default.weight', 'layers.14.mlp.up_proj.lora_A.default.weight', 'layers.14.mlp.up_proj.lora_B.default.weight', 'layers.14.self_attn.k_proj.lora_A.default.weight', 'layers.14.self_attn.k_proj.lora_B.default.weight', 'layers.14.self_attn.o_proj.lora_A.default.weight', 'layers.14.self_attn.o_proj.lora_B.default.weight', 'layers.14.self_attn.q_proj.lora_A.default.weight', 'layers.14.self_attn.q_proj.lora_B.default.weight', 'layers.14.self_attn.v_proj.lora_A.default.weight', 'layers.14.self_attn.v_proj.lora_B.default.weight', 'layers.15.mlp.down_proj.lora_A.default.weight', 'layers.15.mlp.down_proj.lora_B.default.weight', 'layers.15.mlp.gate_proj.lora_A.default.weight', 'layers.15.mlp.gate_proj.lora_B.default.weight', 'layers.15.mlp.up_proj.lora_A.default.weight', 'layers.15.mlp.up_proj.lora_B.default.weight', 'layers.15.self_attn.k_proj.lora_A.default.weight', 'layers.15.self_attn.k_proj.lora_B.default.weight', 'layers.15.self_attn.o_proj.lora_A.default.weight', 'layers.15.self_attn.o_proj.lora_B.default.weight', 'layers.15.self_attn.q_proj.lora_A.default.weight', 'layers.15.self_attn.q_proj.lora_B.default.weight', 'layers.15.self_attn.v_proj.lora_A.default.weight', 'layers.15.self_attn.v_proj.lora_B.default.weight', 'layers.2.mlp.down_proj.lora_A.default.weight', 'layers.2.mlp.down_proj.lora_B.default.weight', 'layers.2.mlp.gate_proj.lora_A.default.weight', 'layers.2.mlp.gate_proj.lora_B.default.weight', 'layers.2.mlp.up_proj.lora_A.default.weight', 'layers.2.mlp.up_proj.lora_B.default.weight', 'layers.2.self_attn.k_proj.lora_A.default.weight', 'layers.2.self_attn.k_proj.lora_B.default.weight', 'layers.2.self_attn.o_proj.lora_A.default.weight', 'layers.2.self_attn.o_proj.lora_B.default.weight', 'layers.2.self_attn.q_proj.lora_A.default.weight', 'layers.2.self_attn.q_proj.lora_B.default.weight', 'layers.2.self_attn.v_proj.lora_A.default.weight', 'layers.2.self_attn.v_proj.lora_B.default.weight', 'layers.3.mlp.down_proj.lora_A.default.weight', 'layers.3.mlp.down_proj.lora_B.default.weight', 'layers.3.mlp.gate_proj.lora_A.default.weight', 'layers.3.mlp.gate_proj.lora_B.default.weight', 'layers.3.mlp.up_proj.lora_A.default.weight', 'layers.3.mlp.up_proj.lora_B.default.weight', 'layers.3.self_attn.k_proj.lora_A.default.weight', 'layers.3.self_attn.k_proj.lora_B.default.weight', 'layers.3.self_attn.o_proj.lora_A.default.weight', 'layers.3.self_attn.o_proj.lora_B.default.weight', 'layers.3.self_attn.q_proj.lora_A.default.weight', 'layers.3.self_attn.q_proj.lora_B.default.weight', 'layers.3.self_attn.v_proj.lora_A.default.weight', 'layers.3.self_attn.v_proj.lora_B.default.weight', 'layers.4.mlp.down_proj.lora_A.default.weight', 'layers.4.mlp.down_proj.lora_B.default.weight', 'layers.4.mlp.gate_proj.lora_A.default.weight', 'layers.4.mlp.gate_proj.lora_B.default.weight', 'layers.4.mlp.up_proj.lora_A.default.weight', 'layers.4.mlp.up_proj.lora_B.default.weight', 'layers.4.self_attn.k_proj.lora_A.default.weight', 'layers.4.self_attn.k_proj.lora_B.default.weight', 'layers.4.self_attn.o_proj.lora_A.default.weight', 'layers.4.self_attn.o_proj.lora_B.default.weight', 'layers.4.self_attn.q_proj.lora_A.default.weight', 'layers.4.self_attn.q_proj.lora_B.default.weight', 'layers.4.self_attn.v_proj.lora_A.default.weight', 'layers.4.self_attn.v_proj.lora_B.default.weight', 'layers.5.mlp.down_proj.lora_A.default.weight', 'layers.5.mlp.down_proj.lora_B.default.weight', 'layers.5.mlp.gate_proj.lora_A.default.weight', 'layers.5.mlp.gate_proj.lora_B.default.weight', 'layers.5.mlp.up_proj.lora_A.default.weight', 'layers.5.mlp.up_proj.lora_B.default.weight', 'layers.5.self_attn.k_proj.lora_A.default.weight', 'layers.5.self_attn.k_proj.lora_B.default.weight', 'layers.5.self_attn.o_proj.lora_A.default.weight', 'layers.5.self_attn.o_proj.lora_B.default.weight', 'layers.5.self_attn.q_proj.lora_A.default.weight', 'layers.5.self_attn.q_proj.lora_B.default.weight', 'layers.5.self_attn.v_proj.lora_A.default.weight', 'layers.5.self_attn.v_proj.lora_B.default.weight', 'layers.6.mlp.down_proj.lora_A.default.weight', 'layers.6.mlp.down_proj.lora_B.default.weight', 'layers.6.mlp.gate_proj.lora_A.default.weight', 'layers.6.mlp.gate_proj.lora_B.default.weight', 'layers.6.mlp.up_proj.lora_A.default.weight', 'layers.6.mlp.up_proj.lora_B.default.weight', 'layers.6.self_attn.k_proj.lora_A.default.weight', 'layers.6.self_attn.k_proj.lora_B.default.weight', 'layers.6.self_attn.o_proj.lora_A.default.weight', 'layers.6.self_attn.o_proj.lora_B.default.weight', 'layers.6.self_attn.q_proj.lora_A.default.weight', 'layers.6.self_attn.q_proj.lora_B.default.weight', 'layers.6.self_attn.v_proj.lora_A.default.weight', 'layers.6.self_attn.v_proj.lora_B.default.weight', 'layers.7.mlp.down_proj.lora_A.default.weight', 'layers.7.mlp.down_proj.lora_B.default.weight', 'layers.7.mlp.gate_proj.lora_A.default.weight', 'layers.7.mlp.gate_proj.lora_B.default.weight', 'layers.7.mlp.up_proj.lora_A.default.weight', 'layers.7.mlp.up_proj.lora_B.default.weight', 'layers.7.self_attn.k_proj.lora_A.default.weight', 'layers.7.self_attn.k_proj.lora_B.default.weight', 'layers.7.self_attn.o_proj.lora_A.default.weight', 'layers.7.self_attn.o_proj.lora_B.default.weight', 'layers.7.self_attn.q_proj.lora_A.default.weight', 'layers.7.self_attn.q_proj.lora_B.default.weight', 'layers.7.self_attn.v_proj.lora_A.default.weight', 'layers.7.self_attn.v_proj.lora_B.default.weight', 'layers.8.mlp.down_proj.lora_A.default.weight', 'layers.8.mlp.down_proj.lora_B.default.weight', 'layers.8.mlp.gate_proj.lora_A.default.weight', 'layers.8.mlp.gate_proj.lora_B.default.weight', 'layers.8.mlp.up_proj.lora_A.default.weight', 'layers.8.mlp.up_proj.lora_B.default.weight', 'layers.8.self_attn.k_proj.lora_A.default.weight', 'layers.8.self_attn.k_proj.lora_B.default.weight', 'layers.8.self_attn.o_proj.lora_A.default.weight', 'layers.8.self_attn.o_proj.lora_B.default.weight', 'layers.8.self_attn.q_proj.lora_A.default.weight', 'layers.8.self_attn.q_proj.lora_B.default.weight', 'layers.8.self_attn.v_proj.lora_A.default.weight', 'layers.8.self_attn.v_proj.lora_B.default.weight', 'layers.9.mlp.down_proj.lora_A.default.weight', 'layers.9.mlp.down_proj.lora_B.default.weight', 'layers.9.mlp.gate_proj.lora_A.default.weight', 'layers.9.mlp.gate_proj.lora_B.default.weight', 'layers.9.mlp.up_proj.lora_A.default.weight', 'layers.9.mlp.up_proj.lora_B.default.weight', 'layers.9.self_attn.k_proj.lora_A.default.weight', 'layers.9.self_attn.k_proj.lora_B.default.weight', 'layers.9.self_attn.o_proj.lora_A.default.weight', 'layers.9.self_attn.o_proj.lora_B.default.weight', 'layers.9.self_attn.q_proj.lora_A.default.weight', 'layers.9.self_attn.q_proj.lora_B.default.weight', 'layers.9.self_attn.v_proj.lora_A.default.weight', 'layers.9.self_attn.v_proj.lora_B.default.weight']. \n",
            "2024-11-03:19:07:10,361 WARNING  [model.py:422] model.chat_template was called with the chat_template set to False or None. Therefore no chat template will be applied. Make sure this is an intended behavior.\n",
            "2024-11-03:19:07:10,363 INFO     [task.py:415] Building contexts for hellaswag on rank 0...\n",
            "\r  0%|          | 0/10042 [00:00<?, ?it/s]\r  2%|▏         | 226/10042 [00:00<00:04, 2259.85it/s]\r  5%|▍         | 459/10042 [00:00<00:04, 2296.92it/s]\r  7%|▋         | 689/10042 [00:00<00:04, 2279.66it/s]\r  9%|▉         | 922/10042 [00:00<00:03, 2296.55it/s]\r 12%|█▏        | 1156/10042 [00:00<00:03, 2309.81it/s]\r 14%|█▍        | 1391/10042 [00:00<00:03, 2320.06it/s]\r 16%|█▌        | 1626/10042 [00:00<00:03, 2328.83it/s]\r 19%|█▊        | 1864/10042 [00:00<00:03, 2341.26it/s]\r 21%|██        | 2101/10042 [00:00<00:03, 2348.33it/s]\r 23%|██▎       | 2336/10042 [00:01<00:03, 2342.62it/s]\r 26%|██▌       | 2571/10042 [00:01<00:03, 2344.71it/s]\r 28%|██▊       | 2808/10042 [00:01<00:03, 2351.39it/s]\r 30%|███       | 3044/10042 [00:01<00:02, 2353.31it/s]\r 33%|███▎      | 3280/10042 [00:01<00:02, 2352.08it/s]\r 35%|███▌      | 3516/10042 [00:01<00:02, 2352.67it/s]\r 37%|███▋      | 3752/10042 [00:01<00:02, 2348.99it/s]\r 40%|███▉      | 3987/10042 [00:01<00:02, 2348.90it/s]\r 42%|████▏     | 4223/10042 [00:01<00:02, 2351.46it/s]\r 44%|████▍     | 4460/10042 [00:01<00:02, 2354.97it/s]\r 47%|████▋     | 4696/10042 [00:02<00:02, 2351.98it/s]\r 49%|████▉     | 4932/10042 [00:02<00:02, 2349.53it/s]\r 51%|█████▏    | 5168/10042 [00:02<00:02, 2350.74it/s]\r 54%|█████▍    | 5404/10042 [00:02<00:01, 2348.32it/s]\r 56%|█████▌    | 5639/10042 [00:02<00:01, 2346.07it/s]\r 58%|█████▊    | 5874/10042 [00:02<00:01, 2346.27it/s]\r 61%|██████    | 6109/10042 [00:02<00:01, 2345.75it/s]\r 63%|██████▎   | 6345/10042 [00:02<00:01, 2347.82it/s]\r 66%|██████▌   | 6580/10042 [00:02<00:01, 2345.66it/s]\r 68%|██████▊   | 6815/10042 [00:02<00:01, 2345.94it/s]\r 70%|███████   | 7050/10042 [00:03<00:01, 2347.08it/s]\r 73%|███████▎  | 7285/10042 [00:03<00:01, 2345.63it/s]\r 75%|███████▍  | 7521/10042 [00:03<00:01, 2347.10it/s]\r 77%|███████▋  | 7757/10042 [00:03<00:00, 2348.00it/s]\r 80%|███████▉  | 7992/10042 [00:03<00:00, 2347.02it/s]\r 82%|████████▏ | 8227/10042 [00:03<00:00, 2341.77it/s]\r 84%|████████▍ | 8462/10042 [00:03<00:00, 2334.41it/s]\r 87%|████████▋ | 8696/10042 [00:03<00:00, 2333.20it/s]\r 89%|████████▉ | 8931/10042 [00:03<00:00, 2336.42it/s]\r 91%|█████████▏| 9165/10042 [00:03<00:00, 2337.23it/s]\r 94%|█████████▎| 9400/10042 [00:04<00:00, 2337.02it/s]\r 96%|█████████▌| 9635/10042 [00:04<00:00, 2339.20it/s]\r 98%|█████████▊| 9870/10042 [00:04<00:00, 2341.07it/s]\r100%|██████████| 10042/10042 [00:04<00:00, 2340.90it/s]\n",
            "2024-11-03:19:07:16,106 INFO     [evaluator.py:489] Running loglikelihood requests\n",
            "\rRunning loglikelihood requests:   0%|          | 0/40168 [00:00<?, ?it/s]\rRunning loglikelihood requests:   0%|          | 1/40168 [00:18<211:10:13, 18.93s/it]\rRunning loglikelihood requests:   0%|          | 66/40168 [00:21<2:43:06,  4.10it/s] \rRunning loglikelihood requests:   0%|          | 130/40168 [00:24<1:25:29,  7.81it/s]\rRunning loglikelihood requests:   0%|          | 195/40168 [00:27<59:00, 11.29it/s]  \rRunning loglikelihood requests:   1%|          | 259/40168 [00:30<46:55, 14.17it/s]\rRunning loglikelihood requests:   1%|          | 323/40168 [00:32<40:36, 16.35it/s]\rRunning loglikelihood requests:   1%|          | 387/40168 [00:35<36:49, 18.01it/s]\rRunning loglikelihood requests:   1%|          | 451/40168 [00:38<33:53, 19.53it/s]\rRunning loglikelihood requests:   1%|▏         | 515/40168 [00:41<32:00, 20.65it/s]\rRunning loglikelihood requests:   1%|▏         | 579/40168 [00:43<31:07, 21.20it/s]\rRunning loglikelihood requests:   2%|▏         | 643/40168 [00:46<30:31, 21.58it/s]\rRunning loglikelihood requests:   2%|▏         | 707/40168 [00:49<30:07, 21.83it/s]\rRunning loglikelihood requests:   2%|▏         | 771/40168 [00:52<29:20, 22.38it/s]\rRunning loglikelihood requests:   2%|▏         | 835/40168 [00:55<28:47, 22.77it/s]\rRunning loglikelihood requests:   2%|▏         | 899/40168 [00:57<28:26, 23.02it/s]\rRunning loglikelihood requests:   2%|▏         | 963/40168 [01:00<28:07, 23.23it/s]\rRunning loglikelihood requests:   3%|▎         | 1027/40168 [01:03<27:56, 23.34it/s]\rRunning loglikelihood requests:   3%|▎         | 1091/40168 [01:05<27:51, 23.38it/s]\rRunning loglikelihood requests:   3%|▎         | 1155/40168 [01:08<27:46, 23.42it/s]\rRunning loglikelihood requests:   3%|▎         | 1219/40168 [01:11<28:05, 23.10it/s]\rRunning loglikelihood requests:   3%|▎         | 1283/40168 [01:14<28:20, 22.87it/s]\rRunning loglikelihood requests:   3%|▎         | 1347/40168 [01:17<28:28, 22.73it/s]\rRunning loglikelihood requests:   4%|▎         | 1411/40168 [01:20<28:30, 22.65it/s]\rRunning loglikelihood requests:   4%|▎         | 1475/40168 [01:22<28:30, 22.62it/s]\rRunning loglikelihood requests:   4%|▍         | 1539/40168 [01:25<28:24, 22.66it/s]\rRunning loglikelihood requests:   4%|▍         | 1603/40168 [01:28<28:18, 22.70it/s]\rRunning loglikelihood requests:   4%|▍         | 1667/40168 [01:31<28:13, 22.74it/s]\rRunning loglikelihood requests:   4%|▍         | 1731/40168 [01:34<28:06, 22.80it/s]\rRunning loglikelihood requests:   4%|▍         | 1795/40168 [01:36<28:01, 22.82it/s]\rRunning loglikelihood requests:   5%|▍         | 1859/40168 [01:39<27:24, 23.30it/s]\rRunning loglikelihood requests:   5%|▍         | 1923/40168 [01:42<26:55, 23.68it/s]\rRunning loglikelihood requests:   5%|▍         | 1987/40168 [01:44<26:33, 23.96it/s]\rRunning loglikelihood requests:   5%|▌         | 2051/40168 [01:47<26:17, 24.16it/s]\rRunning loglikelihood requests:   5%|▌         | 2115/40168 [01:49<26:04, 24.32it/s]\rRunning loglikelihood requests:   5%|▌         | 2179/40168 [01:52<25:54, 24.44it/s]\rRunning loglikelihood requests:   6%|▌         | 2243/40168 [01:55<25:44, 24.56it/s]\rRunning loglikelihood requests:   6%|▌         | 2307/40168 [01:57<25:36, 24.64it/s]\rRunning loglikelihood requests:   6%|▌         | 2371/40168 [02:00<25:31, 24.68it/s]\rRunning loglikelihood requests:   6%|▌         | 2435/40168 [02:02<25:25, 24.74it/s]\rRunning loglikelihood requests:   6%|▌         | 2499/40168 [02:05<25:21, 24.75it/s]\rRunning loglikelihood requests:   6%|▋         | 2563/40168 [02:07<25:12, 24.87it/s]\rRunning loglikelihood requests:   7%|▋         | 2627/40168 [02:10<25:06, 24.93it/s]\rRunning loglikelihood requests:   7%|▋         | 2691/40168 [02:13<24:59, 24.99it/s]\rRunning loglikelihood requests:   7%|▋         | 2755/40168 [02:15<24:55, 25.01it/s]\rRunning loglikelihood requests:   7%|▋         | 2819/40168 [02:18<24:51, 25.04it/s]\rRunning loglikelihood requests:   7%|▋         | 2883/40168 [02:20<24:49, 25.03it/s]\rRunning loglikelihood requests:   7%|▋         | 2947/40168 [02:23<24:44, 25.07it/s]\rRunning loglikelihood requests:   7%|▋         | 3011/40168 [02:25<24:40, 25.09it/s]\rRunning loglikelihood requests:   8%|▊         | 3075/40168 [02:28<24:36, 25.12it/s]\rRunning loglikelihood requests:   8%|▊         | 3139/40168 [02:30<24:33, 25.14it/s]\rRunning loglikelihood requests:   8%|▊         | 3203/40168 [02:33<24:30, 25.13it/s]\rRunning loglikelihood requests:   8%|▊         | 3267/40168 [02:35<24:29, 25.12it/s]\rRunning loglikelihood requests:   8%|▊         | 3331/40168 [02:38<24:27, 25.10it/s]\rRunning loglikelihood requests:   8%|▊         | 3395/40168 [02:41<24:18, 25.22it/s]\rRunning loglikelihood requests:   9%|▊         | 3459/40168 [02:43<24:11, 25.29it/s]\rRunning loglikelihood requests:   9%|▉         | 3523/40168 [02:46<24:06, 25.34it/s]\rRunning loglikelihood requests:   9%|▉         | 3587/40168 [02:48<24:02, 25.36it/s]\rRunning loglikelihood requests:   9%|▉         | 3651/40168 [02:51<23:59, 25.37it/s]\rRunning loglikelihood requests:   9%|▉         | 3715/40168 [02:53<23:56, 25.37it/s]\rRunning loglikelihood requests:   9%|▉         | 3779/40168 [02:56<23:54, 25.37it/s]\rRunning loglikelihood requests:  10%|▉         | 3843/40168 [02:58<24:05, 25.12it/s]\rRunning loglikelihood requests:  10%|▉         | 3907/40168 [03:01<24:12, 24.97it/s]\rRunning loglikelihood requests:  10%|▉         | 3971/40168 [03:03<24:16, 24.86it/s]\rRunning loglikelihood requests:  10%|█         | 4035/40168 [03:06<24:18, 24.77it/s]\rRunning loglikelihood requests:  10%|█         | 4099/40168 [03:09<24:18, 24.73it/s]\rRunning loglikelihood requests:  10%|█         | 4163/40168 [03:11<24:17, 24.71it/s]\rRunning loglikelihood requests:  11%|█         | 4227/40168 [03:14<24:15, 24.69it/s]\rRunning loglikelihood requests:  11%|█         | 4291/40168 [03:16<23:53, 25.03it/s]\rRunning loglikelihood requests:  11%|█         | 4355/40168 [03:19<23:36, 25.28it/s]\rRunning loglikelihood requests:  11%|█         | 4419/40168 [03:21<23:23, 25.46it/s]\rRunning loglikelihood requests:  11%|█         | 4483/40168 [03:24<23:14, 25.59it/s]\rRunning loglikelihood requests:  11%|█▏        | 4547/40168 [03:26<23:05, 25.70it/s]\rRunning loglikelihood requests:  11%|█▏        | 4611/40168 [03:29<22:59, 25.77it/s]\rRunning loglikelihood requests:  12%|█▏        | 4675/40168 [03:31<22:54, 25.82it/s]\rRunning loglikelihood requests:  12%|█▏        | 4739/40168 [03:34<22:48, 25.89it/s]\rRunning loglikelihood requests:  12%|█▏        | 4803/40168 [03:36<22:43, 25.95it/s]\rRunning loglikelihood requests:  12%|█▏        | 4867/40168 [03:38<22:37, 26.00it/s]\rRunning loglikelihood requests:  12%|█▏        | 4931/40168 [03:41<22:34, 26.02it/s]\rRunning loglikelihood requests:  12%|█▏        | 4995/40168 [03:43<22:29, 26.06it/s]\rRunning loglikelihood requests:  13%|█▎        | 5060/40168 [03:46<22:19, 26.21it/s]\rRunning loglikelihood requests:  13%|█▎        | 5124/40168 [03:48<22:18, 26.18it/s]\rRunning loglikelihood requests:  13%|█▎        | 5188/40168 [03:51<22:17, 26.16it/s]\rRunning loglikelihood requests:  13%|█▎        | 5252/40168 [03:53<22:09, 26.27it/s]\rRunning loglikelihood requests:  13%|█▎        | 5316/40168 [03:56<22:04, 26.32it/s]\rRunning loglikelihood requests:  13%|█▎        | 5380/40168 [03:58<21:58, 26.38it/s]\rRunning loglikelihood requests:  14%|█▎        | 5444/40168 [04:00<21:54, 26.41it/s]\rRunning loglikelihood requests:  14%|█▎        | 5508/40168 [04:03<21:52, 26.41it/s]\rRunning loglikelihood requests:  14%|█▍        | 5572/40168 [04:05<21:48, 26.44it/s]\rRunning loglikelihood requests:  14%|█▍        | 5636/40168 [04:08<21:45, 26.45it/s]\rRunning loglikelihood requests:  14%|█▍        | 5700/40168 [04:10<21:41, 26.48it/s]\rRunning loglikelihood requests:  14%|█▍        | 5764/40168 [04:13<21:57, 26.11it/s]\rRunning loglikelihood requests:  15%|█▍        | 5828/40168 [04:15<22:07, 25.86it/s]\rRunning loglikelihood requests:  15%|█▍        | 5892/40168 [04:18<22:14, 25.69it/s]\rRunning loglikelihood requests:  15%|█▍        | 5956/40168 [04:20<22:18, 25.56it/s]\rRunning loglikelihood requests:  15%|█▍        | 6020/40168 [04:23<22:20, 25.46it/s]\rRunning loglikelihood requests:  15%|█▌        | 6084/40168 [04:25<22:20, 25.43it/s]\rRunning loglikelihood requests:  15%|█▌        | 6148/40168 [04:28<22:20, 25.38it/s]\rRunning loglikelihood requests:  15%|█▌        | 6212/40168 [04:30<22:18, 25.36it/s]\rRunning loglikelihood requests:  16%|█▌        | 6276/40168 [04:33<22:17, 25.33it/s]\rRunning loglikelihood requests:  16%|█▌        | 6340/40168 [04:35<21:51, 25.80it/s]\rRunning loglikelihood requests:  16%|█▌        | 6404/40168 [04:38<21:33, 26.11it/s]\rRunning loglikelihood requests:  16%|█▌        | 6468/40168 [04:40<21:19, 26.34it/s]\rRunning loglikelihood requests:  16%|█▋        | 6532/40168 [04:42<21:08, 26.51it/s]\rRunning loglikelihood requests:  16%|█▋        | 6597/40168 [04:45<20:55, 26.74it/s]\rRunning loglikelihood requests:  17%|█▋        | 6661/40168 [04:47<20:50, 26.79it/s]\rRunning loglikelihood requests:  17%|█▋        | 6725/40168 [04:50<20:46, 26.82it/s]\rRunning loglikelihood requests:  17%|█▋        | 6789/40168 [04:52<20:43, 26.84it/s]\rRunning loglikelihood requests:  17%|█▋        | 6853/40168 [04:54<20:57, 26.50it/s]\rRunning loglikelihood requests:  17%|█▋        | 6917/40168 [04:57<21:05, 26.28it/s]\rRunning loglikelihood requests:  17%|█▋        | 6982/40168 [04:59<21:04, 26.24it/s]\rRunning loglikelihood requests:  18%|█▊        | 7046/40168 [05:02<21:10, 26.08it/s]\rRunning loglikelihood requests:  18%|█▊        | 7110/40168 [05:04<21:12, 25.97it/s]\rRunning loglikelihood requests:  18%|█▊        | 7174/40168 [05:07<21:14, 25.90it/s]\rRunning loglikelihood requests:  18%|█▊        | 7238/40168 [05:09<21:14, 25.83it/s]\rRunning loglikelihood requests:  18%|█▊        | 7302/40168 [05:12<21:14, 25.79it/s]\rRunning loglikelihood requests:  18%|█▊        | 7366/40168 [05:14<21:11, 25.79it/s]\rRunning loglikelihood requests:  18%|█▊        | 7430/40168 [05:17<21:10, 25.78it/s]\rRunning loglikelihood requests:  19%|█▊        | 7494/40168 [05:19<20:46, 26.22it/s]\rRunning loglikelihood requests:  19%|█▉        | 7558/40168 [05:21<20:28, 26.55it/s]\rRunning loglikelihood requests:  19%|█▉        | 7622/40168 [05:24<20:14, 26.79it/s]\rRunning loglikelihood requests:  19%|█▉        | 7686/40168 [05:26<20:04, 26.96it/s]\rRunning loglikelihood requests:  19%|█▉        | 7750/40168 [05:28<19:57, 27.08it/s]\rRunning loglikelihood requests:  19%|█▉        | 7814/40168 [05:31<19:50, 27.17it/s]\rRunning loglikelihood requests:  20%|█▉        | 7878/40168 [05:33<19:46, 27.22it/s]\rRunning loglikelihood requests:  20%|█▉        | 7942/40168 [05:35<19:42, 27.26it/s]\rRunning loglikelihood requests:  20%|█▉        | 8006/40168 [05:38<19:38, 27.29it/s]\rRunning loglikelihood requests:  20%|██        | 8070/40168 [05:40<19:59, 26.75it/s]\rRunning loglikelihood requests:  20%|██        | 8134/40168 [05:43<20:14, 26.37it/s]\rRunning loglikelihood requests:  20%|██        | 8198/40168 [05:45<20:23, 26.13it/s]\rRunning loglikelihood requests:  21%|██        | 8262/40168 [05:48<20:29, 25.95it/s]\rRunning loglikelihood requests:  21%|██        | 8326/40168 [05:50<20:33, 25.81it/s]\rRunning loglikelihood requests:  21%|██        | 8390/40168 [05:53<20:35, 25.72it/s]\rRunning loglikelihood requests:  21%|██        | 8454/40168 [05:55<20:35, 25.67it/s]\rRunning loglikelihood requests:  21%|██        | 8518/40168 [05:58<20:34, 25.64it/s]\rRunning loglikelihood requests:  21%|██▏       | 8582/40168 [06:00<20:32, 25.62it/s]\rRunning loglikelihood requests:  22%|██▏       | 8646/40168 [06:03<20:14, 25.96it/s]\rRunning loglikelihood requests:  22%|██▏       | 8711/40168 [06:05<19:55, 26.30it/s]\rRunning loglikelihood requests:  22%|██▏       | 8775/40168 [06:08<19:49, 26.40it/s]\rRunning loglikelihood requests:  22%|██▏       | 8839/40168 [06:10<19:43, 26.48it/s]\rRunning loglikelihood requests:  22%|██▏       | 8903/40168 [06:12<19:37, 26.54it/s]\rRunning loglikelihood requests:  22%|██▏       | 8969/40168 [06:15<19:23, 26.82it/s]\rRunning loglikelihood requests:  22%|██▏       | 9034/40168 [06:17<19:16, 26.91it/s]\rRunning loglikelihood requests:  23%|██▎       | 9098/40168 [06:20<19:17, 26.84it/s]\rRunning loglikelihood requests:  23%|██▎       | 9162/40168 [06:22<19:16, 26.81it/s]\rRunning loglikelihood requests:  23%|██▎       | 9226/40168 [06:24<19:15, 26.77it/s]\rRunning loglikelihood requests:  23%|██▎       | 9290/40168 [06:27<19:12, 26.80it/s]\rRunning loglikelihood requests:  23%|██▎       | 9354/40168 [06:29<19:08, 26.83it/s]\rRunning loglikelihood requests:  23%|██▎       | 9418/40168 [06:31<19:05, 26.85it/s]\rRunning loglikelihood requests:  24%|██▎       | 9482/40168 [06:34<19:03, 26.84it/s]\rRunning loglikelihood requests:  24%|██▍       | 9546/40168 [06:36<18:59, 26.87it/s]\rRunning loglikelihood requests:  24%|██▍       | 9611/40168 [06:39<18:51, 27.00it/s]\rRunning loglikelihood requests:  24%|██▍       | 9675/40168 [06:41<18:50, 26.97it/s]\rRunning loglikelihood requests:  24%|██▍       | 9739/40168 [06:43<18:48, 26.96it/s]\rRunning loglikelihood requests:  24%|██▍       | 9803/40168 [06:46<18:46, 26.94it/s]\rRunning loglikelihood requests:  25%|██▍       | 9867/40168 [06:48<18:46, 26.91it/s]\rRunning loglikelihood requests:  25%|██▍       | 9931/40168 [06:50<18:25, 27.36it/s]\rRunning loglikelihood requests:  25%|██▍       | 9994/40168 [07:01<18:22, 27.36it/s]\rRunning loglikelihood requests:  25%|██▍       | 9995/40168 [07:04<44:01, 11.42it/s]\rRunning loglikelihood requests:  25%|██▌       | 10059/40168 [07:06<36:02, 13.93it/s]\rRunning loglikelihood requests:  25%|██▌       | 10123/40168 [07:08<30:27, 16.44it/s]\rRunning loglikelihood requests:  25%|██▌       | 10187/40168 [07:10<26:31, 18.83it/s]\rRunning loglikelihood requests:  26%|██▌       | 10251/40168 [07:13<23:46, 20.97it/s]\rRunning loglikelihood requests:  26%|██▌       | 10316/40168 [07:15<21:44, 22.88it/s]\rRunning loglikelihood requests:  26%|██▌       | 10380/40168 [07:17<20:25, 24.30it/s]\rRunning loglikelihood requests:  26%|██▌       | 10444/40168 [07:19<19:29, 25.42it/s]\rRunning loglikelihood requests:  26%|██▌       | 10508/40168 [07:22<18:48, 26.27it/s]\rRunning loglikelihood requests:  26%|██▋       | 10572/40168 [07:24<18:20, 26.90it/s]\rRunning loglikelihood requests:  26%|██▋       | 10636/40168 [07:26<18:06, 27.18it/s]\rRunning loglikelihood requests:  27%|██▋       | 10701/40168 [07:28<17:51, 27.49it/s]\rRunning loglikelihood requests:  27%|██▋       | 10765/40168 [07:31<17:45, 27.61it/s]\rRunning loglikelihood requests:  27%|██▋       | 10829/40168 [07:33<17:39, 27.68it/s]\rRunning loglikelihood requests:  27%|██▋       | 10893/40168 [07:35<17:35, 27.73it/s]\rRunning loglikelihood requests:  27%|██▋       | 10957/40168 [07:38<17:31, 27.78it/s]\rRunning loglikelihood requests:  27%|██▋       | 11021/40168 [07:40<17:29, 27.78it/s]\rRunning loglikelihood requests:  28%|██▊       | 11085/40168 [07:42<17:25, 27.81it/s]\rRunning loglikelihood requests:  28%|██▊       | 11149/40168 [07:45<17:23, 27.82it/s]\rRunning loglikelihood requests:  28%|██▊       | 11213/40168 [07:47<17:20, 27.83it/s]\rRunning loglikelihood requests:  28%|██▊       | 11277/40168 [07:49<17:04, 28.20it/s]\rRunning loglikelihood requests:  28%|██▊       | 11341/40168 [07:51<16:52, 28.46it/s]\rRunning loglikelihood requests:  28%|██▊       | 11405/40168 [07:53<16:44, 28.64it/s]\rRunning loglikelihood requests:  29%|██▊       | 11469/40168 [07:56<16:37, 28.78it/s]\rRunning loglikelihood requests:  29%|██▊       | 11533/40168 [07:58<16:32, 28.86it/s]\rRunning loglikelihood requests:  29%|██▉       | 11597/40168 [08:00<16:27, 28.94it/s]\rRunning loglikelihood requests:  29%|██▉       | 11661/40168 [08:02<16:22, 29.00it/s]\rRunning loglikelihood requests:  29%|██▉       | 11725/40168 [08:04<16:19, 29.03it/s]\rRunning loglikelihood requests:  29%|██▉       | 11789/40168 [08:07<16:17, 29.03it/s]\rRunning loglikelihood requests:  30%|██▉       | 11853/40168 [08:09<16:14, 29.05it/s]\rRunning loglikelihood requests:  30%|██▉       | 11917/40168 [08:11<16:11, 29.07it/s]\rRunning loglikelihood requests:  30%|██▉       | 11981/40168 [08:13<16:21, 28.73it/s]\rRunning loglikelihood requests:  30%|██▉       | 12045/40168 [08:16<16:26, 28.50it/s]\rRunning loglikelihood requests:  30%|███       | 12109/40168 [08:18<16:30, 28.31it/s]\rRunning loglikelihood requests:  30%|███       | 12173/40168 [08:20<16:31, 28.23it/s]\rRunning loglikelihood requests:  30%|███       | 12237/40168 [08:22<16:33, 28.13it/s]\rRunning loglikelihood requests:  31%|███       | 12301/40168 [08:25<16:32, 28.09it/s]\rRunning loglikelihood requests:  31%|███       | 12366/40168 [08:27<16:28, 28.14it/s]\rRunning loglikelihood requests:  31%|███       | 12430/40168 [08:29<16:27, 28.08it/s]\rRunning loglikelihood requests:  31%|███       | 12494/40168 [08:32<16:28, 28.00it/s]\rRunning loglikelihood requests:  31%|███▏      | 12558/40168 [08:34<16:27, 27.96it/s]\rRunning loglikelihood requests:  31%|███▏      | 12622/40168 [08:36<16:05, 28.52it/s]\rRunning loglikelihood requests:  32%|███▏      | 12686/40168 [08:38<15:50, 28.92it/s]\rRunning loglikelihood requests:  32%|███▏      | 12750/40168 [08:40<15:38, 29.20it/s]\rRunning loglikelihood requests:  32%|███▏      | 12814/40168 [08:42<15:29, 29.42it/s]\rRunning loglikelihood requests:  32%|███▏      | 12878/40168 [08:45<15:23, 29.56it/s]\rRunning loglikelihood requests:  32%|███▏      | 12942/40168 [08:47<15:17, 29.68it/s]\rRunning loglikelihood requests:  32%|███▏      | 13007/40168 [08:49<15:08, 29.88it/s]\rRunning loglikelihood requests:  33%|███▎      | 13071/40168 [08:51<15:06, 29.89it/s]\rRunning loglikelihood requests:  33%|███▎      | 13135/40168 [08:53<15:04, 29.89it/s]\rRunning loglikelihood requests:  33%|███▎      | 13199/40168 [08:55<15:01, 29.91it/s]\rRunning loglikelihood requests:  33%|███▎      | 13263/40168 [08:57<14:58, 29.94it/s]\rRunning loglikelihood requests:  33%|███▎      | 13327/40168 [09:00<14:55, 29.98it/s]\rRunning loglikelihood requests:  33%|███▎      | 13391/40168 [09:02<14:52, 29.99it/s]\rRunning loglikelihood requests:  33%|███▎      | 13455/40168 [09:04<14:50, 30.01it/s]\rRunning loglikelihood requests:  34%|███▎      | 13519/40168 [09:06<14:47, 30.02it/s]\rRunning loglikelihood requests:  34%|███▍      | 13583/40168 [09:08<14:44, 30.04it/s]\rRunning loglikelihood requests:  34%|███▍      | 13648/40168 [09:10<14:38, 30.18it/s]\rRunning loglikelihood requests:  34%|███▍      | 13713/40168 [09:12<14:33, 30.30it/s]\rRunning loglikelihood requests:  34%|███▍      | 13777/40168 [09:14<14:32, 30.23it/s]\rRunning loglikelihood requests:  34%|███▍      | 13841/40168 [09:17<14:31, 30.20it/s]\rRunning loglikelihood requests:  35%|███▍      | 13905/40168 [09:19<14:28, 30.25it/s]\rRunning loglikelihood requests:  35%|███▍      | 13969/40168 [09:21<14:24, 30.32it/s]\rRunning loglikelihood requests:  35%|███▍      | 14033/40168 [09:23<14:21, 30.34it/s]\rRunning loglikelihood requests:  35%|███▌      | 14097/40168 [09:25<14:18, 30.38it/s]\rRunning loglikelihood requests:  35%|███▌      | 14161/40168 [09:27<14:15, 30.39it/s]\rRunning loglikelihood requests:  35%|███▌      | 14225/40168 [09:29<14:12, 30.42it/s]\rRunning loglikelihood requests:  36%|███▌      | 14289/40168 [09:31<14:11, 30.41it/s]\rRunning loglikelihood requests:  36%|███▌      | 14353/40168 [09:33<14:07, 30.45it/s]\rRunning loglikelihood requests:  36%|███▌      | 14417/40168 [09:36<14:05, 30.44it/s]\rRunning loglikelihood requests:  36%|███▌      | 14481/40168 [09:38<14:03, 30.46it/s]\rRunning loglikelihood requests:  36%|███▌      | 14545/40168 [09:40<14:01, 30.44it/s]\rRunning loglikelihood requests:  36%|███▋      | 14609/40168 [09:42<14:09, 30.10it/s]\rRunning loglikelihood requests:  37%|███▋      | 14673/40168 [09:44<14:14, 29.83it/s]\rRunning loglikelihood requests:  37%|███▋      | 14737/40168 [09:46<14:16, 29.67it/s]\rRunning loglikelihood requests:  37%|███▋      | 14801/40168 [09:48<14:19, 29.53it/s]\rRunning loglikelihood requests:  37%|███▋      | 14866/40168 [09:51<14:15, 29.57it/s]\rRunning loglikelihood requests:  37%|███▋      | 14930/40168 [09:53<14:16, 29.47it/s]\rRunning loglikelihood requests:  37%|███▋      | 14994/40168 [09:55<14:16, 29.40it/s]\rRunning loglikelihood requests:  37%|███▋      | 15059/40168 [09:57<14:11, 29.49it/s]\rRunning loglikelihood requests:  38%|███▊      | 15123/40168 [09:59<14:11, 29.41it/s]\rRunning loglikelihood requests:  38%|███▊      | 15187/40168 [10:02<14:10, 29.36it/s]\rRunning loglikelihood requests:  38%|███▊      | 15251/40168 [10:04<14:09, 29.31it/s]\rRunning loglikelihood requests:  38%|███▊      | 15315/40168 [10:06<14:04, 29.43it/s]\rRunning loglikelihood requests:  38%|███▊      | 15379/40168 [10:08<14:00, 29.50it/s]\rRunning loglikelihood requests:  38%|███▊      | 15443/40168 [10:10<13:58, 29.48it/s]\rRunning loglikelihood requests:  39%|███▊      | 15507/40168 [10:12<13:55, 29.52it/s]\rRunning loglikelihood requests:  39%|███▉      | 15571/40168 [10:15<13:52, 29.53it/s]\rRunning loglikelihood requests:  39%|███▉      | 15636/40168 [10:17<13:45, 29.70it/s]\rRunning loglikelihood requests:  39%|███▉      | 15700/40168 [10:19<13:44, 29.69it/s]\rRunning loglikelihood requests:  39%|███▉      | 15765/40168 [10:21<13:38, 29.82it/s]\rRunning loglikelihood requests:  39%|███▉      | 15829/40168 [10:23<13:37, 29.75it/s]\rRunning loglikelihood requests:  40%|███▉      | 15893/40168 [10:25<13:36, 29.73it/s]\rRunning loglikelihood requests:  40%|███▉      | 15957/40168 [10:28<13:35, 29.68it/s]\rRunning loglikelihood requests:  40%|███▉      | 16021/40168 [10:30<13:39, 29.47it/s]\rRunning loglikelihood requests:  40%|████      | 16085/40168 [10:32<13:41, 29.31it/s]\rRunning loglikelihood requests:  40%|████      | 16149/40168 [10:34<13:41, 29.24it/s]\rRunning loglikelihood requests:  40%|████      | 16213/40168 [10:36<13:41, 29.15it/s]\rRunning loglikelihood requests:  41%|████      | 16277/40168 [10:39<13:40, 29.12it/s]\rRunning loglikelihood requests:  41%|████      | 16341/40168 [10:41<13:39, 29.07it/s]\rRunning loglikelihood requests:  41%|████      | 16405/40168 [10:43<13:38, 29.04it/s]\rRunning loglikelihood requests:  41%|████      | 16469/40168 [10:45<13:36, 29.02it/s]\rRunning loglikelihood requests:  41%|████      | 16533/40168 [10:47<13:34, 29.01it/s]\rRunning loglikelihood requests:  41%|████▏     | 16597/40168 [10:50<13:33, 28.97it/s]\rRunning loglikelihood requests:  41%|████▏     | 16661/40168 [10:52<13:09, 29.78it/s]\rRunning loglikelihood requests:  42%|████▏     | 16725/40168 [10:54<12:52, 30.35it/s]\rRunning loglikelihood requests:  42%|████▏     | 16789/40168 [10:56<12:39, 30.79it/s]\rRunning loglikelihood requests:  42%|████▏     | 16853/40168 [10:58<12:30, 31.08it/s]\rRunning loglikelihood requests:  42%|████▏     | 16917/40168 [11:00<12:22, 31.32it/s]\rRunning loglikelihood requests:  42%|████▏     | 16981/40168 [11:02<12:16, 31.47it/s]\rRunning loglikelihood requests:  42%|████▏     | 17045/40168 [11:04<12:11, 31.61it/s]\rRunning loglikelihood requests:  43%|████▎     | 17109/40168 [11:06<12:08, 31.67it/s]\rRunning loglikelihood requests:  43%|████▎     | 17173/40168 [11:08<12:04, 31.74it/s]\rRunning loglikelihood requests:  43%|████▎     | 17237/40168 [11:10<12:02, 31.75it/s]\rRunning loglikelihood requests:  43%|████▎     | 17301/40168 [11:12<11:59, 31.80it/s]\rRunning loglikelihood requests:  43%|████▎     | 17365/40168 [11:14<12:06, 31.39it/s]\rRunning loglikelihood requests:  43%|████▎     | 17429/40168 [11:16<12:10, 31.14it/s]\rRunning loglikelihood requests:  44%|████▎     | 17493/40168 [11:18<12:12, 30.97it/s]\rRunning loglikelihood requests:  44%|████▎     | 17557/40168 [11:20<12:12, 30.86it/s]\rRunning loglikelihood requests:  44%|████▍     | 17621/40168 [11:22<12:12, 30.76it/s]\rRunning loglikelihood requests:  44%|████▍     | 17685/40168 [11:24<12:12, 30.71it/s]\rRunning loglikelihood requests:  44%|████▍     | 17749/40168 [11:26<12:12, 30.62it/s]\rRunning loglikelihood requests:  44%|████▍     | 17813/40168 [11:29<12:10, 30.62it/s]\rRunning loglikelihood requests:  45%|████▍     | 17877/40168 [11:31<12:08, 30.61it/s]\rRunning loglikelihood requests:  45%|████▍     | 17941/40168 [11:33<12:06, 30.60it/s]\rRunning loglikelihood requests:  45%|████▍     | 18005/40168 [11:35<11:58, 30.86it/s]\rRunning loglikelihood requests:  45%|████▍     | 18069/40168 [11:37<11:51, 31.08it/s]\rRunning loglikelihood requests:  45%|████▌     | 18133/40168 [11:39<11:46, 31.19it/s]\rRunning loglikelihood requests:  45%|████▌     | 18197/40168 [11:41<11:41, 31.31it/s]\rRunning loglikelihood requests:  45%|████▌     | 18261/40168 [11:43<11:38, 31.36it/s]\rRunning loglikelihood requests:  46%|████▌     | 18325/40168 [11:45<11:34, 31.43it/s]\rRunning loglikelihood requests:  46%|████▌     | 18389/40168 [11:47<11:32, 31.45it/s]\rRunning loglikelihood requests:  46%|████▌     | 18453/40168 [11:49<11:29, 31.50it/s]\rRunning loglikelihood requests:  46%|████▌     | 18517/40168 [11:51<11:27, 31.49it/s]\rRunning loglikelihood requests:  46%|████▋     | 18581/40168 [11:53<11:24, 31.52it/s]\rRunning loglikelihood requests:  46%|████▋     | 18645/40168 [11:55<11:23, 31.50it/s]\rRunning loglikelihood requests:  47%|████▋     | 18709/40168 [11:57<11:28, 31.17it/s]\rRunning loglikelihood requests:  47%|████▋     | 18773/40168 [11:59<11:31, 30.94it/s]\rRunning loglikelihood requests:  47%|████▋     | 18837/40168 [12:01<11:32, 30.81it/s]\rRunning loglikelihood requests:  47%|████▋     | 18901/40168 [12:03<11:33, 30.69it/s]\rRunning loglikelihood requests:  47%|████▋     | 18965/40168 [12:06<11:32, 30.62it/s]\rRunning loglikelihood requests:  47%|████▋     | 19029/40168 [12:08<11:32, 30.53it/s]\rRunning loglikelihood requests:  48%|████▊     | 19093/40168 [12:10<11:30, 30.52it/s]\rRunning loglikelihood requests:  48%|████▊     | 19157/40168 [12:12<11:29, 30.47it/s]\rRunning loglikelihood requests:  48%|████▊     | 19221/40168 [12:14<11:27, 30.46it/s]\rRunning loglikelihood requests:  48%|████▊     | 19285/40168 [12:16<11:08, 31.26it/s]\rRunning loglikelihood requests:  48%|████▊     | 19349/40168 [12:18<10:53, 31.87it/s]\rRunning loglikelihood requests:  48%|████▊     | 19413/40168 [12:20<10:43, 32.27it/s]\rRunning loglikelihood requests:  48%|████▊     | 19477/40168 [12:22<10:35, 32.58it/s]\rRunning loglikelihood requests:  49%|████▊     | 19541/40168 [12:24<10:28, 32.80it/s]\rRunning loglikelihood requests:  49%|████▉     | 19605/40168 [12:26<10:23, 32.97it/s]\rRunning loglikelihood requests:  49%|████▉     | 19669/40168 [12:27<10:19, 33.07it/s]\rRunning loglikelihood requests:  49%|████▉     | 19733/40168 [12:29<10:16, 33.16it/s]\rRunning loglikelihood requests:  49%|████▉     | 19797/40168 [12:31<10:13, 33.21it/s]\rRunning loglikelihood requests:  49%|████▉     | 19861/40168 [12:33<10:10, 33.29it/s]\rRunning loglikelihood requests:  50%|████▉     | 19925/40168 [12:35<10:12, 33.04it/s]\rRunning loglikelihood requests:  50%|████▉     | 19989/40168 [12:37<10:12, 32.92it/s]\rRunning loglikelihood requests:  50%|████▉     | 20053/40168 [12:39<10:12, 32.83it/s]\rRunning loglikelihood requests:  50%|█████     | 20117/40168 [12:41<10:11, 32.78it/s]\rRunning loglikelihood requests:  50%|█████     | 20181/40168 [12:43<10:10, 32.74it/s]\rRunning loglikelihood requests:  50%|█████     | 20245/40168 [12:45<10:09, 32.69it/s]\rRunning loglikelihood requests:  51%|█████     | 20309/40168 [12:47<10:08, 32.62it/s]\rRunning loglikelihood requests:  51%|█████     | 20373/40168 [12:49<10:06, 32.64it/s]\rRunning loglikelihood requests:  51%|█████     | 20437/40168 [12:51<09:57, 33.05it/s]\rRunning loglikelihood requests:  51%|█████     | 20501/40168 [12:53<09:49, 33.35it/s]\rRunning loglikelihood requests:  51%|█████     | 20565/40168 [12:55<09:43, 33.58it/s]\rRunning loglikelihood requests:  51%|█████▏    | 20629/40168 [12:56<09:38, 33.76it/s]\rRunning loglikelihood requests:  52%|█████▏    | 20693/40168 [12:58<09:35, 33.84it/s]\rRunning loglikelihood requests:  52%|█████▏    | 20757/40168 [13:00<09:32, 33.91it/s]\rRunning loglikelihood requests:  52%|█████▏    | 20821/40168 [13:02<09:29, 33.94it/s]\rRunning loglikelihood requests:  52%|█████▏    | 20885/40168 [13:04<09:27, 34.00it/s]\rRunning loglikelihood requests:  52%|█████▏    | 20949/40168 [13:06<09:25, 34.01it/s]\rRunning loglikelihood requests:  52%|█████▏    | 21013/40168 [13:08<09:33, 33.38it/s]\rRunning loglikelihood requests:  52%|█████▏    | 21077/40168 [13:10<09:39, 32.94it/s]\rRunning loglikelihood requests:  53%|█████▎    | 21141/40168 [13:12<09:42, 32.67it/s]\rRunning loglikelihood requests:  53%|█████▎    | 21205/40168 [13:14<09:44, 32.45it/s]\rRunning loglikelihood requests:  53%|█████▎    | 21269/40168 [13:16<09:44, 32.32it/s]\rRunning loglikelihood requests:  53%|█████▎    | 21333/40168 [13:18<09:44, 32.21it/s]\rRunning loglikelihood requests:  53%|█████▎    | 21397/40168 [13:20<09:43, 32.16it/s]\rRunning loglikelihood requests:  53%|█████▎    | 21461/40168 [13:22<09:42, 32.10it/s]\rRunning loglikelihood requests:  54%|█████▎    | 21525/40168 [13:24<09:41, 32.08it/s]\rRunning loglikelihood requests:  54%|█████▎    | 21589/40168 [13:26<09:24, 32.92it/s]\rRunning loglikelihood requests:  54%|█████▍    | 21653/40168 [13:27<09:11, 33.58it/s]\rRunning loglikelihood requests:  54%|█████▍    | 21717/40168 [13:29<09:02, 34.01it/s]\rRunning loglikelihood requests:  54%|█████▍    | 21782/40168 [13:31<08:53, 34.45it/s]\rRunning loglikelihood requests:  54%|█████▍    | 21846/40168 [13:33<08:49, 34.63it/s]\rRunning loglikelihood requests:  55%|█████▍    | 21910/40168 [13:35<08:44, 34.78it/s]\rRunning loglikelihood requests:  55%|█████▍    | 21974/40168 [13:37<08:42, 34.85it/s]\rRunning loglikelihood requests:  55%|█████▍    | 22038/40168 [13:38<08:39, 34.91it/s]\rRunning loglikelihood requests:  55%|█████▌    | 22102/40168 [13:40<08:40, 34.71it/s]\rRunning loglikelihood requests:  55%|█████▌    | 22166/40168 [13:42<08:40, 34.60it/s]\rRunning loglikelihood requests:  55%|█████▌    | 22230/40168 [13:44<08:40, 34.48it/s]\rRunning loglikelihood requests:  56%|█████▌    | 22294/40168 [13:46<08:38, 34.46it/s]\rRunning loglikelihood requests:  56%|█████▌    | 22358/40168 [13:48<08:37, 34.39it/s]\rRunning loglikelihood requests:  56%|█████▌    | 22422/40168 [13:50<08:36, 34.37it/s]\rRunning loglikelihood requests:  56%|█████▌    | 22486/40168 [13:51<08:34, 34.37it/s]\rRunning loglikelihood requests:  56%|█████▌    | 22550/40168 [13:53<08:32, 34.36it/s]\rRunning loglikelihood requests:  56%|█████▋    | 22614/40168 [13:55<08:28, 34.53it/s]\rRunning loglikelihood requests:  56%|█████▋    | 22679/40168 [13:57<08:21, 34.87it/s]\rRunning loglikelihood requests:  57%|█████▋    | 22743/40168 [13:59<08:18, 34.95it/s]\rRunning loglikelihood requests:  57%|█████▋    | 22807/40168 [14:01<08:16, 34.98it/s]\rRunning loglikelihood requests:  57%|█████▋    | 22871/40168 [14:02<08:14, 34.95it/s]\rRunning loglikelihood requests:  57%|█████▋    | 22935/40168 [14:04<08:13, 34.94it/s]\rRunning loglikelihood requests:  57%|█████▋    | 22999/40168 [14:06<08:10, 34.97it/s]\rRunning loglikelihood requests:  57%|█████▋    | 23063/40168 [14:08<08:13, 34.66it/s]\rRunning loglikelihood requests:  58%|█████▊    | 23127/40168 [14:10<08:15, 34.40it/s]\rRunning loglikelihood requests:  58%|█████▊    | 23191/40168 [14:12<08:15, 34.28it/s]\rRunning loglikelihood requests:  58%|█████▊    | 23255/40168 [14:14<08:15, 34.15it/s]\rRunning loglikelihood requests:  58%|█████▊    | 23320/40168 [14:16<08:11, 34.29it/s]\rRunning loglikelihood requests:  58%|█████▊    | 23384/40168 [14:17<08:11, 34.17it/s]\rRunning loglikelihood requests:  58%|█████▊    | 23448/40168 [14:19<08:10, 34.08it/s]\rRunning loglikelihood requests:  59%|█████▊    | 23512/40168 [14:21<08:02, 34.54it/s]\rRunning loglikelihood requests:  59%|█████▊    | 23576/40168 [14:23<07:55, 34.86it/s]\rRunning loglikelihood requests:  59%|█████▉    | 23640/40168 [14:25<07:52, 35.00it/s]\rRunning loglikelihood requests:  59%|█████▉    | 23704/40168 [14:27<07:47, 35.18it/s]\rRunning loglikelihood requests:  59%|█████▉    | 23768/40168 [14:28<07:45, 35.26it/s]\rRunning loglikelihood requests:  59%|█████▉    | 23832/40168 [14:30<07:41, 35.37it/s]\rRunning loglikelihood requests:  59%|█████▉    | 23896/40168 [14:32<07:39, 35.41it/s]\rRunning loglikelihood requests:  60%|█████▉    | 23960/40168 [14:34<07:40, 35.22it/s]\rRunning loglikelihood requests:  60%|█████▉    | 24024/40168 [14:36<07:40, 35.05it/s]\rRunning loglikelihood requests:  60%|█████▉    | 24088/40168 [14:37<07:39, 34.97it/s]\rRunning loglikelihood requests:  60%|██████    | 24152/40168 [14:39<07:39, 34.86it/s]\rRunning loglikelihood requests:  60%|██████    | 24216/40168 [14:41<07:37, 34.84it/s]\rRunning loglikelihood requests:  60%|██████    | 24280/40168 [14:43<07:36, 34.83it/s]\rRunning loglikelihood requests:  61%|██████    | 24344/40168 [14:45<07:22, 35.76it/s]\rRunning loglikelihood requests:  61%|██████    | 24408/40168 [14:46<07:12, 36.42it/s]\rRunning loglikelihood requests:  61%|██████    | 24472/40168 [14:48<07:05, 36.90it/s]\rRunning loglikelihood requests:  61%|██████    | 24536/40168 [14:50<06:59, 37.24it/s]\rRunning loglikelihood requests:  61%|██████    | 24600/40168 [14:51<06:55, 37.48it/s]\rRunning loglikelihood requests:  61%|██████▏   | 24664/40168 [14:53<06:52, 37.59it/s]\rRunning loglikelihood requests:  62%|██████▏   | 24728/40168 [14:55<06:47, 37.86it/s]\rRunning loglikelihood requests:  62%|██████▏   | 24792/40168 [14:56<06:44, 38.01it/s]\rRunning loglikelihood requests:  62%|██████▏   | 24856/40168 [14:58<06:42, 38.09it/s]\rRunning loglikelihood requests:  62%|██████▏   | 24920/40168 [15:00<06:40, 38.11it/s]\rRunning loglikelihood requests:  62%|██████▏   | 24984/40168 [15:01<06:37, 38.23it/s]\rRunning loglikelihood requests:  62%|██████▏   | 25048/40168 [15:03<06:32, 38.56it/s]\rRunning loglikelihood requests:  63%|██████▎   | 25112/40168 [15:05<06:28, 38.78it/s]\rRunning loglikelihood requests:  63%|██████▎   | 25176/40168 [15:06<06:25, 38.92it/s]\rRunning loglikelihood requests:  63%|██████▎   | 25240/40168 [15:08<06:22, 39.04it/s]\rRunning loglikelihood requests:  63%|██████▎   | 25304/40168 [15:10<06:20, 39.10it/s]\rRunning loglikelihood requests:  63%|██████▎   | 25368/40168 [15:11<06:23, 38.63it/s]\rRunning loglikelihood requests:  63%|██████▎   | 25432/40168 [15:13<06:24, 38.31it/s]\rRunning loglikelihood requests:  63%|██████▎   | 25496/40168 [15:15<06:24, 38.13it/s]\rRunning loglikelihood requests:  64%|██████▎   | 25560/40168 [15:16<06:24, 37.99it/s]\rRunning loglikelihood requests:  64%|██████▍   | 25624/40168 [15:18<06:23, 37.95it/s]\rRunning loglikelihood requests:  64%|██████▍   | 25688/40168 [15:20<06:15, 38.59it/s]\rRunning loglikelihood requests:  64%|██████▍   | 25752/40168 [15:21<06:09, 39.00it/s]\rRunning loglikelihood requests:  64%|██████▍   | 25816/40168 [15:23<06:05, 39.31it/s]\rRunning loglikelihood requests:  64%|██████▍   | 25880/40168 [15:24<06:01, 39.53it/s]\rRunning loglikelihood requests:  65%|██████▍   | 25944/40168 [15:26<05:58, 39.67it/s]\rRunning loglikelihood requests:  65%|██████▍   | 26008/40168 [15:28<05:55, 39.79it/s]\rRunning loglikelihood requests:  65%|██████▍   | 26072/40168 [15:29<05:53, 39.85it/s]\rRunning loglikelihood requests:  65%|██████▌   | 26136/40168 [15:31<05:51, 39.91it/s]\rRunning loglikelihood requests:  65%|██████▌   | 26200/40168 [15:32<05:47, 40.16it/s]\rRunning loglikelihood requests:  65%|██████▌   | 26264/40168 [15:34<05:44, 40.32it/s]\rRunning loglikelihood requests:  66%|██████▌   | 26328/40168 [15:36<05:42, 40.36it/s]\rRunning loglikelihood requests:  66%|██████▌   | 26392/40168 [15:37<05:41, 40.40it/s]\rRunning loglikelihood requests:  66%|██████▌   | 26456/40168 [15:39<05:39, 40.38it/s]\rRunning loglikelihood requests:  66%|██████▌   | 26520/40168 [15:40<05:37, 40.41it/s]\rRunning loglikelihood requests:  66%|██████▌   | 26584/40168 [15:42<05:35, 40.44it/s]\rRunning loglikelihood requests:  66%|██████▋   | 26648/40168 [15:43<05:34, 40.47it/s]\rRunning loglikelihood requests:  67%|██████▋   | 26712/40168 [15:45<05:28, 40.93it/s]\rRunning loglikelihood requests:  67%|██████▋   | 26776/40168 [15:46<05:24, 41.27it/s]\rRunning loglikelihood requests:  67%|██████▋   | 26840/40168 [15:48<05:21, 41.48it/s]\rRunning loglikelihood requests:  67%|██████▋   | 26904/40168 [15:50<05:23, 41.06it/s]\rRunning loglikelihood requests:  67%|██████▋   | 26968/40168 [15:51<05:22, 40.92it/s]\rRunning loglikelihood requests:  67%|██████▋   | 27032/40168 [15:53<05:22, 40.76it/s]\rRunning loglikelihood requests:  67%|██████▋   | 27096/40168 [15:54<05:17, 41.15it/s]\rRunning loglikelihood requests:  68%|██████▊   | 27160/40168 [15:56<05:14, 41.36it/s]\rRunning loglikelihood requests:  68%|██████▊   | 27224/40168 [15:57<05:11, 41.56it/s]\rRunning loglikelihood requests:  68%|██████▊   | 27288/40168 [15:59<05:08, 41.76it/s]\rRunning loglikelihood requests:  68%|██████▊   | 27352/40168 [16:00<05:05, 41.95it/s]\rRunning loglikelihood requests:  68%|██████▊   | 27416/40168 [16:02<04:57, 42.87it/s]\rRunning loglikelihood requests:  68%|██████▊   | 27480/40168 [16:03<04:51, 43.56it/s]\rRunning loglikelihood requests:  69%|██████▊   | 27544/40168 [16:05<04:46, 44.05it/s]\rRunning loglikelihood requests:  69%|██████▊   | 27608/40168 [16:06<04:46, 43.82it/s]\rRunning loglikelihood requests:  69%|██████▉   | 27672/40168 [16:08<04:46, 43.61it/s]\rRunning loglikelihood requests:  69%|██████▉   | 27736/40168 [16:09<04:42, 44.00it/s]\rRunning loglikelihood requests:  69%|██████▉   | 27800/40168 [16:10<04:39, 44.27it/s]\rRunning loglikelihood requests:  69%|██████▉   | 27864/40168 [16:12<04:36, 44.43it/s]\rRunning loglikelihood requests:  70%|██████▉   | 27928/40168 [16:13<04:34, 44.58it/s]\rRunning loglikelihood requests:  70%|██████▉   | 27992/40168 [16:15<04:32, 44.61it/s]\rRunning loglikelihood requests:  70%|██████▉   | 28056/40168 [16:16<04:25, 45.58it/s]\rRunning loglikelihood requests:  70%|███████   | 28120/40168 [16:17<04:20, 46.29it/s]\rRunning loglikelihood requests:  70%|███████   | 28184/40168 [16:19<04:16, 46.79it/s]\rRunning loglikelihood requests:  70%|███████   | 28248/40168 [16:20<04:15, 46.73it/s]\rRunning loglikelihood requests:  70%|███████   | 28312/40168 [16:21<04:14, 46.64it/s]\rRunning loglikelihood requests:  71%|███████   | 28376/40168 [16:23<04:07, 47.63it/s]\rRunning loglikelihood requests:  71%|███████   | 28440/40168 [16:24<04:02, 48.34it/s]\rRunning loglikelihood requests:  71%|███████   | 28504/40168 [16:25<03:59, 48.76it/s]\rRunning loglikelihood requests:  71%|███████   | 28568/40168 [16:27<03:58, 48.66it/s]\rRunning loglikelihood requests:  71%|███████▏  | 28632/40168 [16:28<03:57, 48.53it/s]\rRunning loglikelihood requests:  71%|███████▏  | 28696/40168 [16:29<03:53, 49.06it/s]\rRunning loglikelihood requests:  72%|███████▏  | 28760/40168 [16:30<03:50, 49.46it/s]\rRunning loglikelihood requests:  72%|███████▏  | 28824/40168 [16:32<03:48, 49.71it/s]\rRunning loglikelihood requests:  72%|███████▏  | 28888/40168 [16:33<03:46, 49.85it/s]\rRunning loglikelihood requests:  72%|███████▏  | 28952/40168 [16:34<03:44, 50.00it/s]\rRunning loglikelihood requests:  72%|███████▏  | 29016/40168 [16:36<03:42, 50.04it/s]\rRunning loglikelihood requests:  72%|███████▏  | 29080/40168 [16:37<03:36, 51.10it/s]\rRunning loglikelihood requests:  73%|███████▎  | 29144/40168 [16:38<03:33, 51.73it/s]\rRunning loglikelihood requests:  73%|███████▎  | 29208/40168 [16:39<03:29, 52.22it/s]\rRunning loglikelihood requests:  73%|███████▎  | 29272/40168 [16:40<03:31, 51.63it/s]\rRunning loglikelihood requests:  73%|███████▎  | 29336/40168 [16:42<03:31, 51.13it/s]\rRunning loglikelihood requests:  73%|███████▎  | 29400/40168 [16:43<03:32, 50.71it/s]\rRunning loglikelihood requests:  73%|███████▎  | 29464/40168 [16:44<03:31, 50.58it/s]\rRunning loglikelihood requests:  74%|███████▎  | 29528/40168 [16:45<03:26, 51.41it/s]\rRunning loglikelihood requests:  74%|███████▎  | 29592/40168 [16:47<03:23, 51.96it/s]\rRunning loglikelihood requests:  74%|███████▍  | 29656/40168 [16:48<03:20, 52.37it/s]\rRunning loglikelihood requests:  74%|███████▍  | 29720/40168 [16:49<03:18, 52.54it/s]\rRunning loglikelihood requests:  74%|███████▍  | 29784/40168 [16:50<03:16, 52.78it/s]\rRunning loglikelihood requests:  74%|███████▍  | 29848/40168 [16:52<03:14, 52.95it/s]\rRunning loglikelihood requests:  74%|███████▍  | 29912/40168 [16:53<03:13, 53.07it/s]\rRunning loglikelihood requests:  75%|███████▍  | 29976/40168 [16:54<03:07, 54.37it/s]\rRunning loglikelihood requests:  75%|███████▍  | 30040/40168 [16:55<03:02, 55.35it/s]\rRunning loglikelihood requests:  75%|███████▍  | 30104/40168 [16:56<02:59, 56.04it/s]\rRunning loglikelihood requests:  75%|███████▌  | 30168/40168 [16:57<02:56, 56.52it/s]\rRunning loglikelihood requests:  75%|███████▌  | 30232/40168 [16:58<02:58, 55.58it/s]\rRunning loglikelihood requests:  75%|███████▌  | 30296/40168 [17:00<02:58, 55.19it/s]\rRunning loglikelihood requests:  76%|███████▌  | 30360/40168 [17:01<02:58, 54.92it/s]\rRunning loglikelihood requests:  76%|███████▌  | 30424/40168 [17:02<02:58, 54.70it/s]\rRunning loglikelihood requests:  76%|███████▌  | 30488/40168 [17:03<02:57, 54.39it/s]\rRunning loglikelihood requests:  76%|███████▌  | 30552/40168 [17:04<02:53, 55.49it/s]\rRunning loglikelihood requests:  76%|███████▌  | 30616/40168 [17:05<02:49, 56.35it/s]\rRunning loglikelihood requests:  76%|███████▋  | 30680/40168 [17:06<02:46, 56.98it/s]\rRunning loglikelihood requests:  77%|███████▋  | 30744/40168 [17:07<02:44, 57.41it/s]\rRunning loglikelihood requests:  77%|███████▋  | 30808/40168 [17:09<02:43, 57.13it/s]\rRunning loglikelihood requests:  77%|███████▋  | 30872/40168 [17:10<02:43, 56.96it/s]\rRunning loglikelihood requests:  77%|███████▋  | 30936/40168 [17:11<02:42, 56.85it/s]\rRunning loglikelihood requests:  77%|███████▋  | 31000/40168 [17:12<02:41, 56.71it/s]\rRunning loglikelihood requests:  77%|███████▋  | 31064/40168 [17:13<02:40, 56.70it/s]\rRunning loglikelihood requests:  77%|███████▋  | 31128/40168 [17:14<02:39, 56.61it/s]\rRunning loglikelihood requests:  78%|███████▊  | 31192/40168 [17:15<02:33, 58.34it/s]\rRunning loglikelihood requests:  78%|███████▊  | 31256/40168 [17:16<02:29, 59.60it/s]\rRunning loglikelihood requests:  78%|███████▊  | 31320/40168 [17:17<02:26, 60.52it/s]\rRunning loglikelihood requests:  78%|███████▊  | 31384/40168 [17:18<02:23, 61.14it/s]\rRunning loglikelihood requests:  78%|███████▊  | 31448/40168 [17:19<02:21, 61.62it/s]\rRunning loglikelihood requests:  78%|███████▊  | 31512/40168 [17:20<02:21, 61.33it/s]\rRunning loglikelihood requests:  79%|███████▊  | 31576/40168 [17:21<02:20, 61.09it/s]\rRunning loglikelihood requests:  79%|███████▉  | 31640/40168 [17:23<02:19, 61.00it/s]\rRunning loglikelihood requests:  79%|███████▉  | 31704/40168 [17:24<02:18, 60.91it/s]\rRunning loglikelihood requests:  79%|███████▉  | 31768/40168 [17:25<02:18, 60.83it/s]\rRunning loglikelihood requests:  79%|███████▉  | 31832/40168 [17:26<02:17, 60.71it/s]\rRunning loglikelihood requests:  79%|███████▉  | 31896/40168 [17:27<02:14, 61.49it/s]\rRunning loglikelihood requests:  80%|███████▉  | 31960/40168 [17:28<02:12, 62.08it/s]\rRunning loglikelihood requests:  80%|███████▉  | 32024/40168 [17:29<02:10, 62.55it/s]\rRunning loglikelihood requests:  80%|███████▉  | 32088/40168 [17:30<02:08, 62.86it/s]\rRunning loglikelihood requests:  80%|████████  | 32152/40168 [17:31<02:07, 63.04it/s]\rRunning loglikelihood requests:  80%|████████  | 32216/40168 [17:32<02:05, 63.16it/s]\rRunning loglikelihood requests:  80%|████████  | 32280/40168 [17:33<02:04, 63.37it/s]\rRunning loglikelihood requests:  81%|████████  | 32344/40168 [17:34<02:03, 63.48it/s]\rRunning loglikelihood requests:  81%|████████  | 32408/40168 [17:35<02:01, 63.68it/s]\rRunning loglikelihood requests:  81%|████████  | 32472/40168 [17:36<02:00, 63.76it/s]\rRunning loglikelihood requests:  81%|████████  | 32536/40168 [17:37<01:59, 63.85it/s]\rRunning loglikelihood requests:  81%|████████  | 32600/40168 [17:38<01:58, 63.92it/s]\rRunning loglikelihood requests:  81%|████████▏ | 32664/40168 [17:39<01:55, 64.78it/s]\rRunning loglikelihood requests:  81%|████████▏ | 32728/40168 [17:40<01:53, 65.33it/s]\rRunning loglikelihood requests:  82%|████████▏ | 32792/40168 [17:41<01:52, 65.73it/s]\rRunning loglikelihood requests:  82%|████████▏ | 32856/40168 [17:42<01:50, 66.03it/s]\rRunning loglikelihood requests:  82%|████████▏ | 32920/40168 [17:43<01:49, 66.25it/s]\rRunning loglikelihood requests:  82%|████████▏ | 32984/40168 [17:43<01:48, 66.38it/s]\rRunning loglikelihood requests:  82%|████████▏ | 33048/40168 [17:44<01:47, 66.46it/s]\rRunning loglikelihood requests:  82%|████████▏ | 33112/40168 [17:45<01:45, 66.65it/s]\rRunning loglikelihood requests:  83%|████████▎ | 33176/40168 [17:46<01:44, 66.77it/s]\rRunning loglikelihood requests:  83%|████████▎ | 33240/40168 [17:47<01:43, 66.97it/s]\rRunning loglikelihood requests:  83%|████████▎ | 33304/40168 [17:48<01:42, 67.08it/s]\rRunning loglikelihood requests:  83%|████████▎ | 33368/40168 [17:49<01:41, 67.17it/s]\rRunning loglikelihood requests:  83%|████████▎ | 33432/40168 [17:50<01:40, 67.15it/s]\rRunning loglikelihood requests:  83%|████████▎ | 33496/40168 [17:51<01:39, 67.16it/s]\rRunning loglikelihood requests:  84%|████████▎ | 33560/40168 [17:52<01:36, 68.13it/s]\rRunning loglikelihood requests:  84%|████████▎ | 33624/40168 [17:53<01:35, 68.82it/s]\rRunning loglikelihood requests:  84%|████████▍ | 33688/40168 [17:54<01:33, 69.26it/s]\rRunning loglikelihood requests:  84%|████████▍ | 33752/40168 [17:55<01:32, 69.67it/s]\rRunning loglikelihood requests:  84%|████████▍ | 33816/40168 [17:56<01:30, 69.82it/s]\rRunning loglikelihood requests:  84%|████████▍ | 33880/40168 [17:57<01:29, 70.08it/s]\rRunning loglikelihood requests:  85%|████████▍ | 33944/40168 [17:57<01:28, 70.16it/s]\rRunning loglikelihood requests:  85%|████████▍ | 34008/40168 [17:58<01:28, 69.90it/s]\rRunning loglikelihood requests:  85%|████████▍ | 34072/40168 [17:59<01:27, 69.55it/s]\rRunning loglikelihood requests:  85%|████████▍ | 34136/40168 [18:00<01:26, 69.44it/s]\rRunning loglikelihood requests:  85%|████████▌ | 34200/40168 [18:01<01:26, 69.28it/s]\rRunning loglikelihood requests:  85%|████████▌ | 34264/40168 [18:02<01:25, 69.15it/s]\rRunning loglikelihood requests:  85%|████████▌ | 34328/40168 [18:03<01:24, 69.05it/s]\rRunning loglikelihood requests:  86%|████████▌ | 34392/40168 [18:04<01:23, 69.01it/s]\rRunning loglikelihood requests:  86%|████████▌ | 34456/40168 [18:05<01:21, 70.41it/s]\rRunning loglikelihood requests:  86%|████████▌ | 34520/40168 [18:06<01:19, 71.47it/s]\rRunning loglikelihood requests:  86%|████████▌ | 34584/40168 [18:07<01:17, 72.20it/s]\rRunning loglikelihood requests:  86%|████████▋ | 34648/40168 [18:07<01:15, 72.69it/s]\rRunning loglikelihood requests:  86%|████████▋ | 34712/40168 [18:08<01:14, 73.11it/s]\rRunning loglikelihood requests:  87%|████████▋ | 34776/40168 [18:09<01:13, 73.40it/s]\rRunning loglikelihood requests:  87%|████████▋ | 34840/40168 [18:10<01:12, 73.58it/s]\rRunning loglikelihood requests:  87%|████████▋ | 34904/40168 [18:11<01:11, 73.81it/s]\rRunning loglikelihood requests:  87%|████████▋ | 34968/40168 [18:12<01:10, 74.04it/s]\rRunning loglikelihood requests:  87%|████████▋ | 35032/40168 [18:13<01:09, 74.16it/s]\rRunning loglikelihood requests:  87%|████████▋ | 35096/40168 [18:13<01:08, 74.18it/s]\rRunning loglikelihood requests:  88%|████████▊ | 35160/40168 [18:14<01:07, 74.31it/s]\rRunning loglikelihood requests:  88%|████████▊ | 35224/40168 [18:15<01:06, 74.41it/s]\rRunning loglikelihood requests:  88%|████████▊ | 35288/40168 [18:16<01:05, 74.40it/s]\rRunning loglikelihood requests:  88%|████████▊ | 35352/40168 [18:17<01:04, 74.39it/s]\rRunning loglikelihood requests:  88%|████████▊ | 35416/40168 [18:18<01:02, 76.28it/s]\rRunning loglikelihood requests:  88%|████████▊ | 35480/40168 [18:18<01:00, 77.60it/s]\rRunning loglikelihood requests:  88%|████████▊ | 35544/40168 [18:19<00:58, 78.50it/s]\rRunning loglikelihood requests:  89%|████████▊ | 35608/40168 [18:20<00:57, 79.25it/s]\rRunning loglikelihood requests:  89%|████████▉ | 35672/40168 [18:21<00:56, 79.78it/s]\rRunning loglikelihood requests:  89%|████████▉ | 35736/40168 [18:22<00:55, 80.02it/s]\rRunning loglikelihood requests:  89%|████████▉ | 35800/40168 [18:22<00:54, 80.36it/s]\rRunning loglikelihood requests:  89%|████████▉ | 35864/40168 [18:23<00:54, 79.67it/s]\rRunning loglikelihood requests:  89%|████████▉ | 35928/40168 [18:24<00:53, 79.20it/s]\rRunning loglikelihood requests:  90%|████████▉ | 35992/40168 [18:25<00:52, 78.97it/s]\rRunning loglikelihood requests:  90%|████████▉ | 36056/40168 [18:26<00:52, 78.83it/s]\rRunning loglikelihood requests:  90%|████████▉ | 36120/40168 [18:27<00:51, 78.62it/s]\rRunning loglikelihood requests:  90%|█████████ | 36184/40168 [18:27<00:50, 78.48it/s]\rRunning loglikelihood requests:  90%|█████████ | 36248/40168 [18:28<00:49, 78.41it/s]\rRunning loglikelihood requests:  90%|█████████ | 36312/40168 [18:29<00:48, 79.55it/s]\rRunning loglikelihood requests:  91%|█████████ | 36376/40168 [18:30<00:47, 80.39it/s]\rRunning loglikelihood requests:  91%|█████████ | 36440/40168 [18:30<00:46, 81.00it/s]\rRunning loglikelihood requests:  91%|█████████ | 36504/40168 [18:31<00:44, 81.51it/s]\rRunning loglikelihood requests:  91%|█████████ | 36568/40168 [18:32<00:43, 81.87it/s]\rRunning loglikelihood requests:  91%|█████████ | 36632/40168 [18:33<00:43, 82.13it/s]\rRunning loglikelihood requests:  91%|█████████▏| 36696/40168 [18:34<00:42, 82.17it/s]\rRunning loglikelihood requests:  92%|█████████▏| 36760/40168 [18:34<00:41, 82.44it/s]\rRunning loglikelihood requests:  92%|█████████▏| 36824/40168 [18:35<00:40, 82.42it/s]\rRunning loglikelihood requests:  92%|█████████▏| 36888/40168 [18:36<00:39, 82.43it/s]\rRunning loglikelihood requests:  92%|█████████▏| 36952/40168 [18:37<00:39, 82.27it/s]\rRunning loglikelihood requests:  92%|█████████▏| 37016/40168 [18:37<00:38, 82.22it/s]\rRunning loglikelihood requests:  92%|█████████▏| 37080/40168 [18:38<00:37, 81.88it/s]\rRunning loglikelihood requests:  92%|█████████▏| 37144/40168 [18:39<00:37, 81.62it/s]\rRunning loglikelihood requests:  93%|█████████▎| 37208/40168 [18:40<00:36, 81.34it/s]\rRunning loglikelihood requests:  93%|█████████▎| 37272/40168 [18:41<00:35, 81.19it/s]\rRunning loglikelihood requests:  93%|█████████▎| 37336/40168 [18:41<00:34, 81.59it/s]\rRunning loglikelihood requests:  93%|█████████▎| 37400/40168 [18:42<00:33, 83.24it/s]\rRunning loglikelihood requests:  93%|█████████▎| 37464/40168 [18:43<00:32, 84.13it/s]\rRunning loglikelihood requests:  93%|█████████▎| 37528/40168 [18:44<00:31, 85.04it/s]\rRunning loglikelihood requests:  94%|█████████▎| 37592/40168 [18:44<00:30, 85.76it/s]\rRunning loglikelihood requests:  94%|█████████▎| 37656/40168 [18:45<00:29, 86.12it/s]\rRunning loglikelihood requests:  94%|█████████▍| 37720/40168 [18:46<00:28, 86.21it/s]\rRunning loglikelihood requests:  94%|█████████▍| 37784/40168 [18:47<00:27, 86.31it/s]\rRunning loglikelihood requests:  94%|█████████▍| 37848/40168 [18:47<00:26, 86.59it/s]\rRunning loglikelihood requests:  94%|█████████▍| 37912/40168 [18:48<00:26, 86.62it/s]\rRunning loglikelihood requests:  95%|█████████▍| 37976/40168 [18:49<00:25, 86.58it/s]\rRunning loglikelihood requests:  95%|█████████▍| 38040/40168 [18:49<00:24, 88.63it/s]\rRunning loglikelihood requests:  95%|█████████▍| 38104/40168 [18:50<00:22, 89.94it/s]\rRunning loglikelihood requests:  95%|█████████▌| 38168/40168 [18:51<00:21, 90.97it/s]\rRunning loglikelihood requests:  95%|█████████▌| 38232/40168 [18:52<00:21, 91.84it/s]\rRunning loglikelihood requests:  95%|█████████▌| 38296/40168 [18:52<00:20, 91.95it/s]\rRunning loglikelihood requests:  95%|█████████▌| 38360/40168 [18:53<00:19, 92.17it/s]\rRunning loglikelihood requests:  96%|█████████▌| 38424/40168 [18:54<00:18, 92.31it/s]\rRunning loglikelihood requests:  96%|█████████▌| 38488/40168 [18:54<00:18, 92.35it/s]\rRunning loglikelihood requests:  96%|█████████▌| 38552/40168 [18:55<00:17, 92.35it/s]\rRunning loglikelihood requests:  96%|█████████▌| 38616/40168 [18:56<00:16, 95.03it/s]\rRunning loglikelihood requests:  96%|█████████▋| 38680/40168 [18:56<00:15, 96.84it/s]\rRunning loglikelihood requests:  96%|█████████▋| 38744/40168 [18:57<00:14, 98.36it/s]\rRunning loglikelihood requests:  97%|█████████▋| 38808/40168 [18:57<00:13, 99.22it/s]\rRunning loglikelihood requests:  97%|█████████▋| 38872/40168 [18:58<00:13, 99.19it/s]\rRunning loglikelihood requests:  97%|█████████▋| 38936/40168 [18:59<00:12, 99.52it/s]\rRunning loglikelihood requests:  97%|█████████▋| 39000/40168 [18:59<00:11, 99.33it/s]\rRunning loglikelihood requests:  97%|█████████▋| 39064/40168 [19:00<00:10, 101.96it/s]\rRunning loglikelihood requests:  97%|█████████▋| 39128/40168 [19:01<00:10, 103.66it/s]\rRunning loglikelihood requests:  98%|█████████▊| 39192/40168 [19:01<00:09, 104.76it/s]\rRunning loglikelihood requests:  98%|█████████▊| 39256/40168 [19:02<00:08, 105.25it/s]\rRunning loglikelihood requests:  98%|█████████▊| 39320/40168 [19:02<00:07, 106.54it/s]\rRunning loglikelihood requests:  98%|█████████▊| 39384/40168 [19:03<00:07, 107.44it/s]\rRunning loglikelihood requests:  98%|█████████▊| 39448/40168 [19:04<00:06, 108.08it/s]\rRunning loglikelihood requests:  98%|█████████▊| 39512/40168 [19:04<00:05, 110.92it/s]\rRunning loglikelihood requests:  99%|█████████▊| 39576/40168 [19:05<00:05, 112.57it/s]\rRunning loglikelihood requests:  99%|█████████▊| 39640/40168 [19:05<00:04, 114.14it/s]\rRunning loglikelihood requests:  99%|█████████▉| 39704/40168 [19:06<00:03, 116.65it/s]\rRunning loglikelihood requests:  99%|█████████▉| 39768/40168 [19:06<00:03, 118.85it/s]\rRunning loglikelihood requests:  99%|█████████▉| 39832/40168 [19:07<00:02, 119.97it/s]\rRunning loglikelihood requests:  99%|█████████▉| 39896/40168 [19:07<00:02, 122.22it/s]\rRunning loglikelihood requests:  99%|█████████▉| 39960/40168 [19:08<00:01, 123.51it/s]\rRunning loglikelihood requests: 100%|█████████▉| 40024/40168 [19:08<00:01, 127.09it/s]\rRunning loglikelihood requests: 100%|█████████▉| 40088/40168 [19:09<00:00, 131.01it/s]\rRunning loglikelihood requests: 100%|█████████▉| 40152/40168 [19:09<00:00, 168.69it/s]\rRunning loglikelihood requests: 100%|██████████| 40168/40168 [19:09<00:00, 34.95it/s] \n",
            "2024-11-03:19:26:50,709 INFO     [evaluation_tracker.py:206] Saving results aggregated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!llama.cpp/main -m /content/llama.cpp/models/7B/ggml-model-q4_0.bin -p \"The quick brown fox jumps over the lazy dog\""
      ],
      "metadata": {
        "id": "G3kEc1OCO-cm",
        "outputId": "2415e2c2-8e72-462a-d80b-89e86621b974",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: llama.cpp/main: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Push the quantized model to Hugging Face Hub"
      ],
      "metadata": {
        "id": "yLGYCvPZfA_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import create_repo, upload_file, login\n",
        "import os"
      ],
      "metadata": {
        "id": "Ks09PIavfAWt"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hf_token = userdata.get('HF_TOKEN')\n",
        "# login(token=hf_token)\n",
        "\n",
        "# repo_id = \"uonyeka-llama-3.2.Instruct_q4_k_m\"\n",
        "\n",
        "# create_repo(repo_id=repo_id, repo_type=\"model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5bp2Q5hkn6N",
        "outputId": "f6065c71-4da8-4c34-a9fb-e0c07a965f0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get HF token from Colab's userdata and login\n",
        "hf_token = userdata.get('hf_token')\n",
        "login(token=hf_token)\n",
        "\n",
        "# Step 1: Create a repository on the Hugging Face Hub with your username\n",
        "username = \"bhuvana-ak7\"  # Your Hugging Face username\n",
        "repo_name = \"OrpoLlama-3.2-1B-V1_q4_k_m\" #OrpoLlama-3.2-1B-V1/ggml-model-Q4_K_M.gguf\n",
        "repo_id = f\"{username}/{repo_name}\"\n",
        "\n",
        "# Create the repository\n",
        "try:\n",
        "    create_repo(\n",
        "        repo_id=repo_id,\n",
        "        repo_type=\"model\",\n",
        "        token=hf_token,\n",
        "        exist_ok=True,\n",
        "        private=True  # Set to False if you want a public repository\n",
        "    )\n",
        "    print(f\"Repository {repo_id} created or already exists\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating repository: {str(e)}\")\n",
        "\n",
        "# Step 2: Define the local directory containing the GGUF model files\n",
        "local_model_dir = \"/content/llama.cpp/models/OrpoLlama-3.2-1B-V1\"\n",
        "\n",
        "# Step 3: Get list of all files in the directory\n",
        "files = os.listdir(local_model_dir)\n",
        "\n",
        "# Step 4: Upload each file\n",
        "for file in files:\n",
        "    # Get the full local path of the file\n",
        "    local_path = os.path.join(local_model_dir, file)\n",
        "\n",
        "    # Skip if it's a directory\n",
        "    if os.path.isdir(local_path):\n",
        "        continue\n",
        "\n",
        "    # Define the path in the repo where the file will be stored\n",
        "    repo_path = file\n",
        "\n",
        "    print(f\"Uploading {file} to repository...\")\n",
        "    try:\n",
        "        # Upload the file to the hub\n",
        "        upload_file(\n",
        "            path_or_fileobj=local_path,\n",
        "            path_in_repo=repo_path,\n",
        "            repo_id=repo_id,\n",
        "            repo_type=\"model\",\n",
        "            token=hf_token\n",
        "        )\n",
        "        print(f\"Successfully uploaded {file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error uploading {file}: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 822,
          "referenced_widgets": [
            "2cf7bf837dfe4f649a7d9c49066bf4db",
            "40f074c51adb4be0a3b710bb568945e7",
            "01317ed07ace452e8c524dd3a117d176",
            "f17c0efedd4a4b95baeef4d3006196c4",
            "b0e622f9db7a4784905a58078c0b9037",
            "1b4426eb0cad4d91be45fb967e19f3ef",
            "1df7c02981c7481ea0aa97e897259f0c",
            "349ccbfb1d7b4d6c9c510429ccc3a4c4",
            "534c3845d3f74e27bed967456f25e8ae",
            "03b2a93eb5bb434f8c183ec953a00dbd",
            "693805b60748439e90da3f9f1a07b61b",
            "85f4cbc3e9a34935bfc925d644d4d439",
            "9b6582ebf2504638a0422988c7cfce27",
            "0c285e47410d4130840a538eb1de4cea",
            "05940046488a4fea8e1460f4756be67c",
            "6a545aed8635413497d581ba3afceb69",
            "9973e2faa704401abca7a9fd90adaf99",
            "7db1143678b1490c950c17018134e56f",
            "e2b89e0438704c4db3375d5c34de53f0",
            "4bf42bfb197249f3b91db5da57835893",
            "6c6c50c4958845b999e092175aac026d",
            "305333d8a8544305ac50b231c55e37a7",
            "5564b927dcd94305884338cf69403cc8",
            "1a536d603b0d4c9884e773bd6c315ae9",
            "e9b26158db0745389149f696f11530b0",
            "6841078a97c74deab6e5bc77821f9ca9",
            "31bb1674034641faa750686f63b70dfd",
            "b88f66e6da91480ba6cbca44c4242fda",
            "cbfc5ade5fee43aabe1de53b72c3753b",
            "ba26934a530347febf417b761cf540c8",
            "913c5484aa60486f9182643ea790222f",
            "82adef309b0d456a9753842c55c969d1",
            "f108fc3b6deb45109524c079811ed6a6",
            "c1fdf167108e46b497b93f1b37553869",
            "7b0de90f9da04637a4c7ec08f18d0f48",
            "7b4cf6cdc25e4c2382d4ef0d3091d0a8",
            "77119c5fbed14bfc9e61646ef2d9178c",
            "8513978404df4a6c98aa9f6abd02e343",
            "70e0bf648c5e486fa3fdc57ffc0041ab",
            "7fdad6eb83bd48db8d890b37fe51e15c",
            "bc6fd94509ef47828415187cb3d70398",
            "b4c811c095344fc694be587c74e5997d",
            "009815dcebcd4dcaa32e6e0034a84ce1",
            "4d9bd1c1e5464378b94ec22b9bb09fe3",
            "55cc7f82933d40138f12068815f5cdd9",
            "6f08a410cf67442b86644d9800ebd500",
            "156b223bad114e75b7635be45175fc1a",
            "978c3ed7a6674fecbf1030d9ccea03cd",
            "c231f768637f4d9b91916050be8904e4",
            "9a26ac1416eb4bcab56fe7dc8c3360d0",
            "1f0f3b72a93d4c21a180ff63a3cb8fe9",
            "9332ebcc614146cc94f31717aeb23491",
            "0be0fe2a10d54432b0bc59b9f5e8d0bf",
            "09f3b5a91a124b28ad0a348e1a6b1c8e",
            "7f504aaee69642118b86acca12da2382"
          ]
        },
        "id": "My1oXLxEn5c7",
        "outputId": "68804663-ec74-4784-b064-c3eac0db212f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n",
            "Repository bhuvana-ak7/OrpoLlama-3.2-1B-V1_q4_k_m created or already exists\n",
            "Uploading generation_config.json to repository...\n",
            "Successfully uploaded generation_config.json\n",
            "Uploading README.md to repository...\n",
            "Successfully uploaded README.md\n",
            "Uploading ggml-model-Q4_K_M.gguf to repository...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ggml-model-Q4_K_M.gguf:   0%|          | 0.00/808M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2cf7bf837dfe4f649a7d9c49066bf4db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully uploaded ggml-model-Q4_K_M.gguf\n",
            "Uploading adapter_model.safetensors to repository...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/45.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85f4cbc3e9a34935bfc925d644d4d439"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully uploaded adapter_model.safetensors\n",
            "Uploading tokenizer.json to repository...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5564b927dcd94305884338cf69403cc8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully uploaded tokenizer.json\n",
            "Uploading .gitattributes to repository...\n",
            "Successfully uploaded .gitattributes\n",
            "Uploading Llama-3.2-1B-V1-F16.gguf to repository...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Llama-3.2-1B-V1-F16.gguf:   0%|          | 0.00/2.48G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1fdf167108e46b497b93f1b37553869"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully uploaded Llama-3.2-1B-V1-F16.gguf\n",
            "Uploading adapter_config.json to repository...\n",
            "Successfully uploaded adapter_config.json\n",
            "Uploading special_tokens_map.json to repository...\n",
            "Successfully uploaded special_tokens_map.json\n",
            "Uploading model.safetensors to repository...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55cc7f82933d40138f12068815f5cdd9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully uploaded model.safetensors\n",
            "Uploading config.json to repository...\n",
            "Successfully uploaded config.json\n",
            "Uploading tokenizer_config.json to repository...\n",
            "Successfully uploaded tokenizer_config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try to generate structured text using the Outlines Library"
      ],
      "metadata": {
        "id": "jCdLyzSpU00u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install email-validator"
      ],
      "metadata": {
        "id": "CcnwSzn0i57g",
        "outputId": "82311845-6fbb-43d3-fc1d-51b69897b9dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting email-validator\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting dnspython>=2.0.0 (from email-validator)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email-validator) (3.10)\n",
            "Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/313.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dnspython, email-validator\n",
            "Successfully installed dnspython-2.7.0 email-validator-2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, EmailStr, HttpUrl, field_validator\n",
        "from outlines import models, generate, types\n",
        "from llama_cpp import Llama\n",
        "import llama_cpp\n",
        "\n",
        "# Load the GGUF Model\n",
        "repo_name = 'bhuvana-ak7/OrpoLlama-3.2-1B-V1_q4_k_m'\n",
        "filename = 'ggml-model-Q4_K_M.gguf'\n",
        "\n",
        "# # Initialize the Llama model from llama-cpp-python\n",
        "# llm = Llama(model_path=llm_path) # Initialize the Llama model with the file path\n",
        "\n",
        "# Initialize the Outlines LlamaCpp model, passing the Llama object\n",
        "model = models.llamacpp(repo_id=repo_name,\n",
        "    filename=filename,\n",
        "    tokenizer=llama_cpp.llama_tokenizer.LlamaHFTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\"))\n",
        "\n",
        "\n",
        "locale = types.locale(\"us\")\n",
        "\n",
        "class Client(BaseModel):\n",
        "    name: str\n",
        "    phone_number: locale.PhoneNumber\n",
        "    zip_code: locale.ZipCode\n",
        "    location: str\n",
        "    Company: str\n",
        "    #website: HttpUrl\n",
        "    #email: EmailStr # Added email field with EmailStr type\n",
        "\n",
        "# Add a validator for the website field\n",
        "    # @field_validator('website')\n",
        "    # def ensure_valid_url(cls, value):\n",
        "    #     \"\"\"\n",
        "    #     This validator ensures the website field is a valid URL,\n",
        "    #     even if it's missing the schema (http or https).\n",
        "    #     \"\"\"\n",
        "    #     if not isinstance(value, str):\n",
        "    #         raise ValueError('Website must be a string')\n",
        "\n",
        "    #     # Add the schema if it's missing\n",
        "    #     if not value.startswith(('http://', 'https://')):\n",
        "    #         value = 'https://' + value\n",
        "\n",
        "    #     return value\n",
        "\n",
        "generator = generate.json(model, Client)"
      ],
      "metadata": {
        "id": "m6hooiQNc_I_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = generator(\n",
        "    \"Create a client profile with the fields person name, phone_number, zip_code, location and company\"\n",
        ")\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQxHEh41YAFD",
        "outputId": "d769a188-541f-4e5b-a4b4-125f7a164371"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name='John' phone_number='077-000-0001' zip_code='23501' location='Parkersville, FL' Company=\"John's Deli\"\n"
          ]
        }
      ]
    }
  ]
}